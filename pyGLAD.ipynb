{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyGLAD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTgeSZf5vR--",
        "outputId": "06117a5d-8f1a-4439-b200-51648d2783ca"
      },
      "source": [
        "!pip install torch-geometric \\\r\n",
        "  torch-sparse==latest+cu101 \\\r\n",
        "  torch-scatter==latest+cu101 \\\r\n",
        "  torch-cluster==latest+cu101 \\\r\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.7.0.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/5c/3e95b76321fb14f24cc2ace392075717f645c4632e796ee0db1bc7d17231/torch_geometric-1.6.3.tar.gz (186kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 5.9MB/s \n",
            "\u001b[?25hCollecting torch-sparse==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (24.3MB)\n",
            "\u001b[K     |████████████████████████████████| 24.3MB 5.5MB/s \n",
            "\u001b[?25hCollecting torch-scatter==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (11.9MB)\n",
            "\u001b[K     |████████████████████████████████| 11.9MB 201kB/s \n",
            "\u001b[?25hCollecting torch-cluster==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.8)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (51.1.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.3-cp36-none-any.whl size=322720 sha256=18ea7f85cfe1cf5c6a046d5ad6f4214bc05dd7964fc4e356910455bad2c9e924\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/47/1e/0af8ce3e21783c3e584c22502011a3367c091694eebc50a971\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric, torch-sparse, torch-scatter, torch-cluster\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-cluster-1.5.8 torch-geometric-1.6.3 torch-scatter-2.0.5 torch-sparse-0.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srbej8jgi10Z",
        "outputId": "d2a90bed-a2ff-4e0f-9470-975ab5b7b606"
      },
      "source": [
        "!pip install karateclub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting karateclub\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/cd/e4e59588a58c6b6f67e5bd561ecfda9bd981632f675e6f8e1e46a8f9fe6e/karateclub-1.0.22.tar.gz (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from karateclub) (1.19.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from karateclub) (2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from karateclub) (4.41.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from karateclub) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from karateclub) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from karateclub) (1.4.1)\n",
            "Collecting pygsp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/89/2f4aa73cccf12bec5179ac5d52a68b508120c838b7e5d456f5ea0c8beade/PyGSP-0.5.1-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.0MB/s \n",
            "\u001b[?25hCollecting gensim==3.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from karateclub) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from karateclub) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->karateclub) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->karateclub) (1.0.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->karateclub) (4.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->karateclub) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->karateclub) (2.8.1)\n",
            "Building wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.0.22-cp36-none-any.whl size=93242 sha256=fb411ec9135ddfa17e5b221ba13dae7f68577158ef7f5e8c4a5c151d9984bd91\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/ef/0a/b9f163e1bc2fa9337ef4b183529d09c2ea78588cbc2c3753ef\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, gensim, karateclub\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3 karateclub-1.0.22 pygsp-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kuRP3AzjBp9",
        "outputId": "4fe4dc46-265e-4be4-e6fd-e592258eaf88"
      },
      "source": [
        "!pip install test_tube"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting test_tube\n",
            "  Downloading https://files.pythonhosted.org/packages/91/f0/5c32f2fbd824f32354f7f4632c957163071597bb2c6a4105f507bc9af7c0/test_tube-0.7.5.tar.gz\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from test_tube) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from test_tube) (1.19.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from test_tube) (2.4.1)\n",
            "Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from test_tube) (2.4.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from test_tube) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from test_tube) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->test_tube) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->test_tube) (2.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.3.0->test_tube) (7.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (51.1.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (0.4.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.15.0->test_tube) (3.3.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->test_tube) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->test_tube) (3.7.4.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15.0->test_tube) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15.0->test_tube) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15.0->test_tube) (4.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test_tube) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test_tube) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test_tube) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15.0->test_tube) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test_tube) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.15.0->test_tube) (3.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15.0->test_tube) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test_tube) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15.0->test_tube) (3.4.0)\n",
            "Building wheels for collected packages: test-tube\n",
            "  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for test-tube: filename=test_tube-0.7.5-cp36-none-any.whl size=25357 sha256=aa7e5237ed11e3da967f83155a76996d3ee8501004f6c543323db8cbc65c2d20\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/52/96/c8b6f3c345cfd3284845ef50818c6996a5658006fe50e40e98\n",
            "Successfully built test-tube\n",
            "Installing collected packages: test-tube\n",
            "Successfully installed test-tube-0.7.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3eiLgAejMW3",
        "outputId": "053cbb63-ecb7-4b82-d765-0213a403810e"
      },
      "source": [
        "!pip install grakel"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting grakel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/90/f3df4acbe2b29d412e45ab5d5d083c5fb05a0174fffa5d1377e49665a7ab/grakel-0.1.8-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 4.1MB/s \n",
            "\u001b[?25hCollecting nose>=1.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 26.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from grakel) (0.29.21)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from grakel) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from grakel) (0.22.2.post1)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from grakel) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from grakel) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from grakel) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->grakel) (1.4.1)\n",
            "Installing collected packages: nose, grakel\n",
            "Successfully installed grakel-0.1.8 nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ENcH4OA2lfL"
      },
      "source": [
        "import pickle\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "random_state = np.random.RandomState(0) # FIX THIS\n",
        "\n",
        "def random_split_counts(total_number, no_of_splits):\n",
        "    split_indices = np.sort(np.random.choice(total_number,no_of_splits-1, replace = False)+1)\n",
        "    split_indices = np.insert(split_indices, 0, 0)\n",
        "    split_indices = np.append(split_indices, total_number)\n",
        "    split_counts = np.diff(split_indices)\n",
        "    return split_counts\n",
        "\n",
        "class GraphGenerator:\n",
        "\tdef __init__(self, numClass):\n",
        "\t\tself.numClass = numClass\n",
        "\n",
        "\tdef format_path(self, G, savePath, graphName, **kwargs):\n",
        "\t\tgraphName = graphName.format(numNode=G.number_of_nodes(), numEdge=G.number_of_edges(), numClass=self.numClass, **kwargs)\n",
        "\t\tsavePath = savePath.format(graphName=graphName, numNode=G.number_of_nodes(), numEdge=G.number_of_edges(), numClass=self.numClass, **kwargs)\n",
        "\t\treturn savePath, graphName\n",
        "\n",
        "\tdef save_graph(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tG_d = nx.to_dict_of_lists(G)\n",
        "\t\tprint(\"Saving graph to {}\".format(os.path.join(savePath, graphName + \".graph\")))\n",
        "\t\tpickle.dump(G_d, open(os.path.join(savePath, graphName + \".graph\"), \"wb\"))\n",
        "\t\n",
        "\tdef save_y(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tally = np.zeros((len(G.nodes()), self.numClass))\n",
        "\t\tfor v in G.nodes():\n",
        "\t\t\tally[v][G.nodes[v]['color'] - 1] = 1\n",
        "\t\tprint(\"Saving labels to {}\".format(os.path.join(savePath, graphName + \".ally\")))\n",
        "\t\tpickle.dump(ally, open(os.path.join(savePath, graphName + \".ally\"), \"wb\"))\n",
        "\t\n",
        "\tdef save_nx_graph(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tprint(\"Pickling networkx graph to {}\".format(os.path.join(savePath, graphName + \".gpickle.gz\")))\n",
        "\t\tnx.write_gpickle(G, os.path.join(savePath, graphName + \".gpickle.gz\"))\n",
        "\n",
        "class MixhopGraphGenerator(GraphGenerator):\n",
        "\tdef get_color(self, class_ratio): # Assign new node to a class\n",
        "\t\tif self.__coloriter:\n",
        "\t\t\treturn next(self.__coloriter)\n",
        "\t\telse:\n",
        "\t\t\treturn np.random.choice(list(range(1, len(class_ratio) + 1)), 1, False, class_ratio)[0]\n",
        "\n",
        "\tdef color_weight(self, col1, col2):\n",
        "\t\tdist = abs(col1 - col2)\n",
        "\t\tdist = min(dist, len(self.classRatio) - dist)\n",
        "\t\treturn self.heteroWeightsDict[dist]\n",
        "\t\n",
        "\tdef get_neighbors(self, G, m, col, h):\n",
        "\t\tpr = dict()\n",
        "\t\tfor v in G.nodes():\n",
        "\t\t\tdegree_v = float(max(G.degree[v], 1)) # Degree is treated as at least 1, so that pr[:] is not all 0\n",
        "\t\t\tif G.nodes[v]['color'] == col:\n",
        "\t\t\t\tpr[v] = float(degree_v) * h\n",
        "\t\t\telse:\n",
        "\t\t\t\tpr[v] = float(degree_v) * ((1 - h) * self.color_weight(col, G.nodes[v]['color']))\n",
        "\n",
        "\t\tnorm_pr = float(sum(pr.values()))\n",
        "\t\tif norm_pr == 0:\n",
        "\t\t\treturn None\n",
        "\t\telse:\n",
        "\t\t\tfor v in list(pr.keys()):\n",
        "\t\t\t\tpr[v] = float(pr[v]) / norm_pr\n",
        "\t\t\tneighbors = np.random.choice(list(pr.keys()), m, False, list(pr.values()))\n",
        "\t\t\treturn neighbors\n",
        "\n",
        "\tdef __init__(self, classRatio, heteroClsWeight=\"circularDist\", **kwargs):\n",
        "\t\tsuper().__init__(len(classRatio))\n",
        "\t\tself.classRatio = classRatio\n",
        "\t\tself.heteroWeightsDict = dict()\n",
        "\t\t\n",
        "\t\tif heteroClsWeight == \"circularDist\":\n",
        "\t\t\tfor i in range(2, self.numClass + 1):\n",
        "\t\t\t\tcircularDist = min(i - 1, self.numClass - (i - 1))\n",
        "\t\t\t\tself.heteroWeightsDict[circularDist] = self.heteroWeightsDict.get(circularDist, 0) + 1\n",
        "\n",
        "\t\t\tmaxDist = max(self.heteroWeightsDict.keys())\n",
        "\t\t\tweightSum = 0\n",
        "\t\t\tfor dist, times in self.heteroWeightsDict.items():\n",
        "\t\t\t\tself.heteroWeightsDict[dist] = kwargs[\"heteroWeightsExponent\"] ** (maxDist - dist)\n",
        "\t\t\t\tweightSum += self.heteroWeightsDict[dist] * times\n",
        "\t\t\tself.heteroWeightsDict = {dist: weight / weightSum for dist, weight in self.heteroWeightsDict.items()}\n",
        "\t\t\n",
        "\t\telif heteroClsWeight == \"uniform\":\n",
        "\t\t\tfor i in range(2, self.numClass + 1):\n",
        "\t\t\t\tcircularDist = min(i - 1, self.numClass - (i - 1))\n",
        "\t\t\t\tself.heteroWeightsDict[circularDist] = 1.0 / len(range(2, self.numClass + 1))\n",
        "\n",
        "\tdef generate_graph(self, n, m, m0, h):\n",
        "\t\t'''\n",
        "\t\tn: Target size for the generated network\n",
        "\t\tm: number of edges added with each new node\n",
        "\t\tm0: number of nodes to begin with\n",
        "\t\th: homophily\n",
        "\t\t'''\n",
        "\t\tif n > 1 and np.sum(self.classRatio) == n:\n",
        "\t\t\t#print(\"Graph will be generated with size of each class exactly equal to the number specified in classRatio.\")\n",
        "\t\t\tself.__colorlist = []\n",
        "\t\t\tfor classID, classSize in enumerate(self.classRatio):\n",
        "\t\t\t\tself.__colorlist += [classID + 1] * int(classSize - m)\n",
        "\t\t\trandom_state.shuffle(self.__colorlist)\n",
        "\t\t\thead_list = list(range(1, self.numClass + 1)) * m\n",
        "\t\t\trandom_state.shuffle(head_list)\n",
        "\t\t\tself.__colorlist = head_list + self.__colorlist\n",
        "\t\t\tself.__coloriter = iter(self.__colorlist)\n",
        "\t\telse:\n",
        "\t\t\tself.__coloriter = None\n",
        "\t\t\n",
        "\t\tif m * self.numClass > m0:\n",
        "\t\t\traise ValueError(\"Barabasi-Albert model requires m to be less or equal to m0\")\n",
        "\n",
        "\t\tif m > n:\n",
        "\t\t\traise ValueError(\"m > n should be satisfied\")\n",
        "\n",
        "\t\tG = nx.Graph()\n",
        "\n",
        "\t\tfor v in range(m0):\n",
        "\t\t\tnext_color = self.get_color(self.classRatio)\n",
        "\t\t\tif v > 1:\n",
        "\t\t\t\tif h != 0 and h != 1:\n",
        "\t\t\t\t\tnext_neighbor = v - 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnext_n = self.get_neighbors(G, 1, next_color, h)\n",
        "\t\t\t\t\tif next_n is not None:\n",
        "\t\t\t\t\t\tnext_neighbor = next_n[0]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnext_neighbor = None\n",
        "\n",
        "\t\t\tG.add_node(v, color=next_color)\n",
        "\t\t\tif v > 1 and next_neighbor is not None:\n",
        "\t\t\t\tG.add_edge(v, next_neighbor)\n",
        "\t\t\t\n",
        "\t\tfor v in range(m0, n):\n",
        "\t\t\tif v % 1000 == 0:\n",
        "\t\t\t\tprint(\"Generating graph... Now processing v = {}\".format(v))\n",
        "\t\t\tcol = self.get_color(self.classRatio)\n",
        "\t\t\tus = self.get_neighbors(G, m, col, h)\n",
        "\n",
        "\t\t\tG.add_node(v, color=col)\n",
        "\t\t\tassert us is not None\n",
        "\t\t\tfor u in us:\n",
        "\t\t\t\tG.add_edge(v, u)\n",
        "\n",
        "\t\tassert len(list(nx.selfloop_edges(G))) == 0\n",
        "\t\treturn G\n",
        "\n",
        "\tdef generate_graph_contaminated(self, n, m, m0, h, contamination = 1.0):\n",
        "\t\t'''\n",
        "\t\tn: Target size for the generated network\n",
        "\t\tm: number of edges added with each new node\n",
        "\t\tm0: number of nodes to begin with\n",
        "\t\th: homophily\n",
        "\t\t'''\n",
        "\t\tif n > 1 and np.sum(self.classRatio) == n:\n",
        "\t\t\t#print(\"Graph will be generated with size of each class exactly equal to the number specified in classRatio.\")\n",
        "\t\t\tself.__colorlist = []\n",
        "\t\t\tfor classID, classSize in enumerate(self.classRatio):\n",
        "\t\t\t\tself.__colorlist += [classID + 1] * int(classSize - m)\n",
        "\t\t\trandom_state.shuffle(self.__colorlist)\n",
        "\t\t\thead_list = list(range(1, self.numClass + 1)) * m\n",
        "\t\t\trandom_state.shuffle(head_list)\n",
        "\t\t\tself.__colorlist = head_list + self.__colorlist\n",
        "\t\t\tself.__coloriter = iter(self.__colorlist)\n",
        "\t\telse:\n",
        "\t\t\tself.__coloriter = None\n",
        "\t\t\n",
        "\t\tif m * self.numClass > m0:\n",
        "\t\t\traise ValueError(\"Barabasi-Albert model requires m to be less or equal to m0\")\n",
        "\n",
        "\t\tif m > n:\n",
        "\t\t\traise ValueError(\"m > n should be satisfied\")\n",
        "\n",
        "\t\tG = nx.Graph()\n",
        "\n",
        "\t\tfor v in range(m0):\n",
        "\t\t\tnext_color = self.get_color(self.classRatio)\n",
        "\t\t\tif v > 1:\n",
        "\t\t\t\tif h != 0 and h != 1:\n",
        "\t\t\t\t\tnext_neighbor = v - 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnext_n = self.get_neighbors(G, 1, next_color, h)\n",
        "\t\t\t\t\tif next_n is not None:\n",
        "\t\t\t\t\t\tnext_neighbor = next_n[0]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnext_neighbor = None\n",
        "\n",
        "\t\t\tG.add_node(v, color=next_color)\n",
        "\t\t\tif v > 1 and next_neighbor is not None:\n",
        "\t\t\t\tG.add_edge(v, next_neighbor)\n",
        "\t\t\t\n",
        "\t\tfor v in range(m0, n):\n",
        "\t\t\tif v % 1000 == 0:\n",
        "\t\t\t\tprint(\"Generating graph... Now processing v = {}\".format(v))\n",
        "\t\t\tcol = self.get_color(self.classRatio)\n",
        "\n",
        "\t\t\tr = np.random.uniform()\n",
        "\t\t\tif r < contamination/2:\n",
        "\t\t\t\tchanged_h = h + 0.4\n",
        "\t\t\telif r < contamination:\n",
        "\t\t\t\tchanged_h = h - 0.4\n",
        "\t\t\telse:\n",
        "\t\t\t\tchanged_h = h\n",
        "\t\t\tus = self.get_neighbors(G, m, col, changed_h)\n",
        "\n",
        "\t\t\tG.add_node(v, color=col)\n",
        "\t\t\tassert us is not None\n",
        "\t\t\tfor u in us:\n",
        "\t\t\t\tG.add_edge(v, u)\n",
        "\n",
        "\t\tassert len(list(nx.selfloop_edges(G))) == 0\n",
        "\t\treturn G\n",
        "\n",
        "\tdef __call__(self, n, m, m0, h):\n",
        "\t\treturn self.generate_graph(n, m, m0, h)\n",
        "\n",
        "\tdef save_graph(self, G:nx.Graph, savePath=\"{graphName}\", graphName=\"{method}-n{numNode}-h{h}-c{numClass}\", **kwargs):\n",
        "\t\tsuper().save_graph(G, savePath, graphName, method=\"mixhop\", **kwargs)\n",
        "\n",
        "\tdef save_y(self, G:nx.Graph, savePath=\"{graphName}\", graphName=\"{method}-n{numNode}-h{h}-c{numClass}\", **kwargs):\n",
        "\t\tsuper().save_y(G, savePath, graphName, method=\"mixhop\", **kwargs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97H7jSOJVy_w"
      },
      "source": [
        "#utils.py\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "from torch_geometric.data import InMemoryDataset\r\n",
        "from torch_geometric.utils import from_networkx\r\n",
        "\r\n",
        "#from mixhop_generator import MixhopGraphGenerator, random_split_counts\r\n",
        "\r\n",
        "\r\n",
        "class SimpleGraphDataset(InMemoryDataset):\r\n",
        "    def __init__(self, name, data_list):\r\n",
        "        self.name = name\r\n",
        "        self.data, self.slices = self.collate(data_list)\r\n",
        "\r\n",
        "        self.__indices__ = None\r\n",
        "        self.transform = None\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_node_labels(self):\r\n",
        "        if self.data.x is None:\r\n",
        "            return 0\r\n",
        "        for i in range(self.data.x.size(1)):\r\n",
        "            x = self.data.x[:, i:]\r\n",
        "            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():\r\n",
        "                return self.data.x.size(1) - i\r\n",
        "        return 0\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_node_attributes(self):\r\n",
        "        if self.data.x is None:\r\n",
        "            return 0\r\n",
        "        return self.data.x.size(1) - self.num_node_labels\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_edge_labels(self):\r\n",
        "        if self.data.edge_attr is None:\r\n",
        "            return 0\r\n",
        "        for i in range(self.data.edge_attr.size(1)):\r\n",
        "            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\r\n",
        "                return self.data.edge_attr.size(1) - i\r\n",
        "        return 0\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_edge_attributes(self):\r\n",
        "        if self.data.edge_attr is None:\r\n",
        "            return 0\r\n",
        "        return self.data.edge_attr.size(1) - self.num_edge_labels\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return '{}({})'.format(self.name, len(self))\r\n",
        "\r\n",
        "\r\n",
        "def load_synthetic_data(num_train=500, num_test_inlier=500, num_test_outlier=25, h_inlier=0.7, h_outlier=0.3, n_min = 50, n_max = 150, no_of_tags = 5, type1 = \"mixhop\", type2 = \"mixhop\", seed = 1213):\r\n",
        "    np.random.seed(seed)\r\n",
        "    print('generating data')\r\n",
        "    g_list = []\r\n",
        "    \r\n",
        "    for i in range(num_train+num_test_inlier):\r\n",
        "        \r\n",
        "        n = np.random.randint(n_min, n_max)\r\n",
        "        \r\n",
        "        if type1 == \"mixhop\":\r\n",
        "            tag_counts = random_split_counts(n, no_of_tags)\r\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0)(n, 2, 10, h_inlier)\r\n",
        "        elif type1 == \"mixhop-contaminated\":\r\n",
        "            tag_counts = random_split_counts(n, no_of_tags)\r\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0).generate_graph_contaminated(n, 2, 10, h_inlier)\r\n",
        "        elif type1 == \"mixhop-disjoint\":\r\n",
        "            tag_counts_1 = random_split_counts(n//2, no_of_tags)\r\n",
        "            g1 = MixhopGraphGenerator(tag_counts_1, heteroWeightsExponent=1.0)(n//2, 2, 10, h_inlier+0.2)\r\n",
        "            tag_counts_2 = random_split_counts(n//2, no_of_tags)\r\n",
        "            g2 = MixhopGraphGenerator(tag_counts_2, heteroWeightsExponent=1.0)(n//2, 2, 10, h_inlier-0.2)\r\n",
        "            g = nx.disjoint_union(g1,g2)\r\n",
        "            #tags = [g.nodes[v]['color'] for v in g.nodes]\r\n",
        "        \r\n",
        "        g = from_networkx(g)\r\n",
        "        g.y = torch.tensor([0])\r\n",
        "\r\n",
        "        g_list.append(g)\r\n",
        "    \r\n",
        "    for i in range(num_test_outlier):\r\n",
        "        \r\n",
        "        n = np.random.randint(n_min, n_max)\r\n",
        "        \r\n",
        "        if type2 == \"mixhop\":\r\n",
        "            tag_counts = random_split_counts(n, no_of_tags)\r\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0)(n, 2, 10, h_outlier)\r\n",
        "        elif type2 == \"mixhop-contaminated\":\r\n",
        "            tag_counts = random_split_counts(n, no_of_tags)\r\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0).generate_graph_contaminated(n, 2, 10, h_outlier)\r\n",
        "        elif type2 == \"mixhop-disjoint\":\r\n",
        "            tag_counts_1 = random_split_counts(n//2, no_of_tags)\r\n",
        "            g1 = MixhopGraphGenerator(tag_counts_1, heteroWeightsExponent=1.0)(n//2, 2, 10, h_outlier+0.2)\r\n",
        "            tag_counts_2 = random_split_counts(n//2, no_of_tags)\r\n",
        "            g2 = MixhopGraphGenerator(tag_counts_2, heteroWeightsExponent=1.0)(n//2, 2, 10, h_outlier-0.2)\r\n",
        "            g = nx.disjoint_union(g1,g2)\r\n",
        "            #tags = [g.nodes[v]['color'] for v in g.nodes]\r\n",
        "        \r\n",
        "        g = from_networkx(g)\r\n",
        "        g.y = torch.tensor([1])\r\n",
        "\r\n",
        "        g_list.append(g)\r\n",
        "    \r\n",
        "    # Extracting unique tags and converting to one-hot features   \r\n",
        "    tagset = set()\r\n",
        "    for g in g_list:\r\n",
        "        tagset = tagset.union(set(g.color.tolist()))\r\n",
        "        \r\n",
        "    tagset = list(tagset)\r\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\r\n",
        "\r\n",
        "    for g in g_list:\r\n",
        "        g.x = torch.zeros(len(g.color.tolist()), len(tagset))\r\n",
        "        g.x[range(len(g.color.tolist())), [tag2index[tag] for tag in g.color.tolist()]] = 1\r\n",
        "        del g.color\r\n",
        "\r\n",
        "    print('Maximum node tag: %d' % len(tagset))\r\n",
        "\r\n",
        "    return SimpleGraphDataset(\"MIXHOP\", g_list)\r\n",
        "\r\n",
        "def mod_CH(X, nu=0.05):\r\n",
        "    X = np.sort(X)\r\n",
        "    \r\n",
        "    outlier_mean = np.mean(X[int((1-nu)*len(X)):])\r\n",
        "    for i in range(int((1-nu)*len(X)), len(X)):\r\n",
        "        X[i] = outlier_mean\r\n",
        "    \r\n",
        "    labels = np.array([0]*int((1-nu)*len(X)) + [1]*(len(X) - int((1-nu)*len(X))))\r\n",
        "    X = X.reshape(-1, 1)\r\n",
        "    from sklearn.metrics import calinski_harabasz_score\r\n",
        "    score = calinski_harabasz_score(X, labels)\r\n",
        "    return score"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNCcz1WZlfzv"
      },
      "source": [
        "#dataloader.py\r\n",
        "\r\n",
        "import torch, os\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from torch_geometric.datasets import TUDataset, GNNBenchmarkDataset\r\n",
        "import torch_geometric.transforms as T\r\n",
        "from torch_geometric.data import DataLoader, DenseDataLoader\r\n",
        "from torch_geometric.utils import degree\r\n",
        "from pathlib import Path\r\n",
        "import math\r\n",
        "import pickle\r\n",
        "\r\n",
        "#from utils import load_synthetic_data\r\n",
        "\r\n",
        "DATA_PATH = 'datasets'\r\n",
        "if not os.path.isdir(DATA_PATH):\r\n",
        "    os.mkdir(DATA_PATH)\r\n",
        "\r\n",
        "class NormalizedDegree(object):\r\n",
        "    def __init__(self, mean, std):\r\n",
        "        self.mean = mean\r\n",
        "        self.std = std\r\n",
        "\r\n",
        "    def __call__(self, data):\r\n",
        "        deg = degree(data.edge_index[0], dtype=torch.float)\r\n",
        "        deg = (deg - self.mean) / self.std\r\n",
        "        data.x = deg.view(-1, 1)\r\n",
        "        return data\r\n",
        "\r\n",
        "class DownsamplingFilter(object):\r\n",
        "    def __init__(self, min_nodes, max_nodes, down_class, down_rate, num_classes, reverse=True, coin=np.random.default_rng()):\r\n",
        "        super(DownsamplingFilter, self).__init__()\r\n",
        "        # if not reverse, downsampling mentioned class, and mentioned class as anomaly class\r\n",
        "        # if reverse, downsampling unmentioned class, and mentioned class as normal class\r\n",
        "        self.min_nodes = min_nodes\r\n",
        "        self.max_nodes = max_nodes\r\n",
        "        self.down_class = down_class\r\n",
        "        self.down_rate = down_rate\r\n",
        "        self.reverse = reverse\r\n",
        "        self.coin = coin\r\n",
        "        \r\n",
        "\r\n",
        "    def __call__(self, data):\r\n",
        "        # step 1: filter the graph node size\r\n",
        "        keep = (data.num_nodes <= self.max_nodes) and (data.num_nodes >= self.min_nodes)\r\n",
        "        # for graph classification, down_rate is 1\r\n",
        "        # downsampling only for anomaly detection, not for classification\r\n",
        "        if self.down_rate == 1:\r\n",
        "            return keep\r\n",
        "        if keep:\r\n",
        "            # step 2: downsampling class\r\n",
        "            mentioned_class = (data.y.item() == self.down_class)\r\n",
        "            \r\n",
        "            anomalous_class = not mentioned_class if self.reverse else mentioned_class\r\n",
        "            data.y.fill_(int(anomalous_class)) # anomalous class as positive\r\n",
        "\r\n",
        "            if anomalous_class:\r\n",
        "                if self.coin.random() > self.down_rate:\r\n",
        "                    keep = False\r\n",
        "        return keep\r\n",
        "\r\n",
        "def load_data(data_name, down_class=0, down_rate=1, dense=False, classify=False, ignore_edge_weight=True, one_class_train=False, seed=1213):\r\n",
        "    np.random.seed(seed)\r\n",
        "    newcoin = np.random.default_rng(seed)\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    \r\n",
        "    if os.path.exists(data_name + \"_\" + str(seed) + \".pkl\"):\r\n",
        "        with open(data_name + \"_\" + str(seed) + \".pkl\", 'rb') as f:\r\n",
        "            dataset_raw = pickle.load(f)\r\n",
        "    elif data_name == 'mixhop':\r\n",
        "        NUM = 500\r\n",
        "        dataset_raw = load_synthetic_data(num_train=NUM, num_test_inlier=NUM, num_test_outlier=int(NUM*down_rate), seed=seed)\r\n",
        "        with open(data_name + \"_\" + str(seed) + \".pkl\", 'wb') as f:\r\n",
        "            pickle.dump(dataset_raw, f)\r\n",
        "    elif data_name == 'mixhop_hard':\r\n",
        "        NUM = 500\r\n",
        "        dataset_raw = load_synthetic_data(num_train=NUM, num_test_inlier=NUM, num_test_outlier=int(NUM*down_rate), seed=seed, type1=\"mixhop-contaminated\", type2=\"mixhop\", h_inlier=0.5, h_outlier=0.5)\r\n",
        "        with open(data_name + \"_\" + str(seed) + \".pkl\", 'wb') as f:\r\n",
        "            pickle.dump(dataset_raw, f)\r\n",
        "    elif data_name == 'mixhop_disjoint':\r\n",
        "        NUM = 500\r\n",
        "        dataset_raw = load_synthetic_data(num_train=NUM, num_test_inlier=NUM, num_test_outlier=int(NUM*down_rate), seed=seed, type1=\"mixhop-disjoint\", type2=\"mixhop\", h_inlier=0.5, h_outlier=0.5)\r\n",
        "        with open(data_name + \"_\" + str(seed) + \".pkl\", 'wb') as f:\r\n",
        "            pickle.dump(dataset_raw, f)\r\n",
        "    elif data_name in ['MNIST', 'CIFAR10']:\r\n",
        "        dataset_raw = GNNBenchmarkDataset(root=DATA_PATH, name=data_name)\r\n",
        "    elif 'ogbg' in data_name:\r\n",
        "        # dataset_raw = PygGraphPropPredDataset(root=DATA_PATH, name=data_name)\r\n",
        "        pass\r\n",
        "        # problem: OGB needs an encoder for transform the discrete feature\r\n",
        "        # if all embeddding are learned to be the same, then it's highly possible\r\n",
        "        # that one-class classification will not be successful\r\n",
        "    else:\r\n",
        "        use_node_attr = True if data_name == 'FRANKENSTEIN' else False\r\n",
        "        dataset_raw = TUDataset(root=DATA_PATH, name=data_name, use_node_attr=use_node_attr)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    # downsampling \r\n",
        "    # 1. Get min and max node and filter them\r\n",
        "    num_nodes_graphs = [data.num_nodes for data in dataset_raw]\r\n",
        "    min_nodes, max_nodes = min(num_nodes_graphs), max(num_nodes_graphs)\r\n",
        "    if max_nodes >= 10000:\r\n",
        "        max_nodes = 10000\r\n",
        "    #print(\"min nodes, max nodes:\", min_nodes, max_nodes)\r\n",
        "    #print(\"#labels: \", dataset_raw.num_classes)\r\n",
        "    \r\n",
        "    \r\n",
        "    # 2. Calculate down_rate for multi_class dataset: h\r\n",
        "    from copy import deepcopy\r\n",
        "    orig_dataset = deepcopy(dataset_raw)\r\n",
        "\r\n",
        "    inliers = [data for data in dataset_raw if data.y == down_class]\r\n",
        "    Ni = len(inliers)\r\n",
        "    No = len(dataset_raw) - Ni\r\n",
        "\r\n",
        "    down_rate = 0.5*down_rate*Ni/No\r\n",
        "    #print(\"down rate:\", down_rate)\r\n",
        "\r\n",
        "    filter = DownsamplingFilter(min_nodes, max_nodes, down_class, down_rate, dataset_raw.num_classes, reverse=True, coin=newcoin)\r\n",
        "    indices = [i for i, data in enumerate(dataset_raw) if filter(data)]\r\n",
        "    #dataset = dataset_raw[torch.tensor(indices)].shuffle()\r\n",
        "    dataset = dataset_raw[torch.tensor(indices)]\r\n",
        "    \r\n",
        "    # report the proportion info of the dataset\r\n",
        "    n = (len(dataset) + 9) // 10\r\n",
        "    #print(\"Downsampled distribution of classes in dataset:\")\r\n",
        "    #labels = np.array([data.y.item() for data in dataset])\r\n",
        "    #label_dist = ['%4d'% (labels==c).sum() for c in range(dataset.num_classes)]\r\n",
        "    #print(\"Dataset: %s, Number of graphs: %d, Class distribution %s, Num of Features %d\"%(\r\n",
        "    #        data_name, len(dataset), label_dist, dataset.num_features))\r\n",
        "\r\n",
        "    # preprocessing: do not use original edge features or weights\r\n",
        "    if ignore_edge_weight:\r\n",
        "        dataset.data.edge_attr = None\r\n",
        "\r\n",
        "    # add transforms which will be conducted when draw each elements from the dataset\r\n",
        "    if dataset.data.x is None:\r\n",
        "        print('using degrees as labels')\r\n",
        "        max_degree = 0\r\n",
        "        degs = []\r\n",
        "        for data in dataset_raw: # ATTENTION: use dataset_raw instead of downsampled version!\r\n",
        "            degs += [degree(data.edge_index[0], dtype=torch.long)]\r\n",
        "            max_degree = max(max_degree, degs[-1].max().item())\r\n",
        "\r\n",
        "        if max_degree < 1000:\r\n",
        "            dataset.transform = T.OneHotDegree(max_degree)\r\n",
        "            # dataset.num_features = max_degree\r\n",
        "        else:\r\n",
        "            # dataset['num_features'] = 1\r\n",
        "            deg = torch.cat(degs, dim=0).to(torch.float)\r\n",
        "            mean, std = deg.mean().item(), deg.std().item()\r\n",
        "            dataset.transform = NormalizedDegree(mean, std)\r\n",
        "    if dense:\r\n",
        "        if dataset.transform is None:\r\n",
        "            dataset.transform = T.ToDense(max_nodes)\r\n",
        "        else:\r\n",
        "            dataset.transform = T.Compose([dataset.transform, T.ToDense(max_nodes)])\r\n",
        "\r\n",
        "    # now let's transform in memory before feed into dataloader to save runtime\r\n",
        "    # because there is no random transformations\r\n",
        "    dataset_list = [data for data in dataset]\r\n",
        "    \r\n",
        "\r\n",
        "    # here is for anomaly detection\r\n",
        "    if not classify and not one_class_train: \r\n",
        "        m = 9 \r\n",
        "        # if data_name == 'ENZYMES': m =7\r\n",
        "        train_dataset = dataset_list[:m*n] # 90% train\r\n",
        "        val_dataset = dataset_list[m*n:]\r\n",
        "        test_dataset = dataset_list\r\n",
        "\r\n",
        "    elif not classify and one_class_train:\r\n",
        "        indices = [i for i, data in enumerate(dataset_list) if data.y.item()==0 and newcoin.random()<0.5]\r\n",
        "        #train_dataset = train_dataset[torch.tensor(indices)] # only keep normal class left\r\n",
        "        train_dataset = [dataset_list[idx] for idx in indices] # only keep normal class left\r\n",
        "        val_dataset = []\r\n",
        "        test_dataset = [dataset_list[idx] for idx in range(len(dataset_list)) if idx not in indices]\r\n",
        "    else:\r\n",
        "        # 10% test, 10% vali, 80% train\r\n",
        "        test_dataset = dataset_list[:n] \r\n",
        "        val_dataset = dataset_list[n:2 * n]\r\n",
        "        train_dataset = dataset_list[2 * n:]\r\n",
        "\r\n",
        "    return train_dataset, val_dataset, test_dataset, dataset, dataset_raw, orig_dataset\r\n",
        "\r\n",
        "\r\n",
        "def create_loaders(data_name, batch_size=32, down_class=0, down_rate=1, dense=False, classify=False, one_class_train=True, data_seed=1213, landmark_seed=0):\r\n",
        "\r\n",
        "    train_dataset, val_dataset, test_dataset, dataset, dataset_raw, orig_dataset = load_data(data_name, \r\n",
        "                                                        down_class=down_class, \r\n",
        "                                                        down_rate=down_rate, \r\n",
        "                                                        dense=dense, \r\n",
        "                                                        classify=classify, \r\n",
        "                                                        one_class_train=one_class_train, seed=data_seed)\r\n",
        "    \r\n",
        "    k = int(5*math.log2(len(train_dataset)))\r\n",
        "    random.seed(landmark_seed)\r\n",
        "    landmark_set = random.sample(train_dataset, k)\r\n",
        "\r\n",
        "    #sizes = [g.x.shape[0] for g in landmark_set]\r\n",
        "    #print(max(sizes))\r\n",
        "\r\n",
        "    '''\r\n",
        "    print(\"Original distribution of classes in dataset:\")\r\n",
        "    labels = np.array([data.y.item() for data in orig_dataset])\r\n",
        "    label_dist = ['%d'% (labels==c).sum() for c in range(orig_dataset.num_classes)]\r\n",
        "    print(\"Dataset: %s, Number of graphs: %d, Class distribution %s, Num of (one-hot encoded) features %d\"%(\r\n",
        "            data_name, len(orig_dataset), label_dist, orig_dataset.num_features))\r\n",
        "    \r\n",
        "    print(\"Initial distribution of classes in dataset:\")\r\n",
        "    labels = np.array([data.y.item() for data in dataset_raw])\r\n",
        "    label_dist = ['%d'% (labels==c).sum() for c in range(dataset_raw.num_classes)]\r\n",
        "    print(\"Dataset: %s, Number of graphs: %d, Class distribution %s, Num of (one-hot encoded) features %d\"%(\r\n",
        "            data_name, len(dataset_raw), label_dist, dataset_raw.num_features))\r\n",
        "    \r\n",
        "    print(\"Downsampled distribution of classes in dataset:\")\r\n",
        "    labels = np.array([data.y.item() for data in dataset])\r\n",
        "    label_dist = ['%d'% (labels==c).sum() for c in [0,1]]\r\n",
        "    print(\"Dataset: %s, Number of graphs: %d, Class distribution %s, Num of (one-hot encoded) features %d\"%(\r\n",
        "            data_name, len(dataset), label_dist, dataset.num_features))\r\n",
        "    '''\r\n",
        "    print(\"After downsampling and test-train splitting, distribution of classes:\")\r\n",
        "    labels = np.array([data.y.item() for data in train_dataset])\r\n",
        "    label_dist = ['%d'% (labels==c).sum() for c in [0,1]]\r\n",
        "    print(\"TRAIN: Number of graphs: %d, Class distribution %s\"%(len(train_dataset), label_dist))\r\n",
        "    \r\n",
        "    #print(\"After downsampling and test-train splitting, distribution of classes in TEST dataset:\")\r\n",
        "    labels = np.array([data.y.item() for data in test_dataset])\r\n",
        "    label_dist = ['%d'% (labels==c).sum() for c in [0,1]]\r\n",
        "    print(\"TEST: Number of graphs: %d, Class distribution %s\"%(len(test_dataset), label_dist))\r\n",
        "    print(\"Number of node features: %d\" %(train_dataset[0].num_features))\r\n",
        "    \r\n",
        "    \r\n",
        "    Loader = DenseDataLoader if dense else DataLoader\r\n",
        "    num_workers = 0\r\n",
        "    train_loader = Loader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\r\n",
        "    test_loader = Loader(test_dataset, batch_size=batch_size, shuffle=False,  pin_memory=True, num_workers=num_workers)\r\n",
        "    landmark_loader = Loader(landmark_set, batch_size=batch_size, shuffle=False,  pin_memory=True, num_workers=num_workers)\r\n",
        "\r\n",
        "    return train_loader, test_loader, landmark_loader, train_dataset[0].num_features"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ejLUfcd40Z"
      },
      "source": [
        "import torch\r\n",
        "import networkx as nx\r\n",
        "from torch_geometric.utils import to_networkx \r\n",
        "\r\n",
        "from karateclub import Graph2Vec, FGSD\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "from sklearn.neighbors import LocalOutlierFactor\r\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
        "\r\n",
        "\r\n",
        "from test_tube import HyperOptArgumentParser\r\n",
        "import logging\r\n",
        "\r\n",
        "\r\n",
        "class EmbeddingBasedGLAD:\r\n",
        "    def __init__(self, embedder, detector, \r\n",
        "                       G2V_nhid=64, G2V_wl_iter=2, \r\n",
        "                       FGSD_hist_bins=200, \r\n",
        "                       IF_n_trees=100, IF_sample_ratio=0.5,\r\n",
        "                       LOF_n_neighbors=20, LOF_n_leaf=30, **kwargs):\r\n",
        "        embedders = {\r\n",
        "            'Graph2Vec': Graph2Vec(wl_iterations=G2V_wl_iter, dimensions=G2V_nhid, \r\n",
        "                                    attributed=True, epochs=50),\r\n",
        "            'FGSD': FGSD(hist_bins=FGSD_hist_bins, hist_range=20)\r\n",
        "        }\r\n",
        "        detectors = {\r\n",
        "            'IF': IsolationForest(n_estimators=IF_n_trees, max_samples=IF_sample_ratio, contamination=0.1),\r\n",
        "            'LOF': LocalOutlierFactor(n_neighbors=LOF_n_neighbors, leaf_size=LOF_n_leaf, contamination=0.1)\r\n",
        "        }\r\n",
        "\r\n",
        "        assert embedder in embedders.keys()\r\n",
        "        assert detector in detectors.keys()\r\n",
        "\r\n",
        "        self.embedder = embedders[embedder]\r\n",
        "        self.detector = detectors[detector]\r\n",
        "        self.embedder_name = embedder\r\n",
        "        self.detector_name = detector\r\n",
        "\r\n",
        "    def __call__(self, dataset):\r\n",
        "        # for inference, output anomaly score\r\n",
        "        dataset = to_networkx_dataset(dataset)\r\n",
        "        self.embedder.fit(dataset)\r\n",
        "        graph_embeddings = self.embedder.get_embedding()\r\n",
        "        self.detector.fit(graph_embeddings)\r\n",
        "\r\n",
        "        if self.detector_name == 'IF':\r\n",
        "            anomaly_scores = -self.detector.decision_function(graph_embeddings)\r\n",
        "        else:\r\n",
        "            anomaly_scores = -self.detector.negative_outlier_factor_\r\n",
        "\r\n",
        "        return anomaly_scores\r\n",
        "\r\n",
        "    def fit(self, dataset):\r\n",
        "        ys = torch.cat([data.y for data in dataset])\r\n",
        "        anomaly_scores = self(dataset)\r\n",
        "\r\n",
        "        roc_auc = roc_auc_score(ys, anomaly_scores)\r\n",
        "        pr_auc = average_precision_score(ys, anomaly_scores)\r\n",
        "        avg_score_normal = anomaly_scores[ys==0].mean()\r\n",
        "        avg_score_abnormal = anomaly_scores[ys==1].mean()  \r\n",
        "\r\n",
        "        metrics = {'roc_auc': roc_auc, \r\n",
        "                   'pr_auc': pr_auc, \r\n",
        "                   'average_score_normal': avg_score_normal,\r\n",
        "                   'average_score_anomaly': avg_score_abnormal}\r\n",
        "        return metrics\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def add_model_specific_args(parent_parser):\r\n",
        "        # nfeat should be inferred from data later\r\n",
        "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser], add_help=False)\r\n",
        "        # model \r\n",
        "        parser.add_argument('--embedder', type=str, default='Graph2Vec')\r\n",
        "        parser.add_argument('--detector', type=str, default='LOF')\r\n",
        "        # Graph2Vec params\r\n",
        "        parser.add_argument('--G2V_nhid', type=int, default=128)\r\n",
        "        parser.add_argument('--G2V_wl_iter', type=int, default=2)\r\n",
        "        # FSGD params\r\n",
        "        parser.add_argument('--FGSD_hist_bins', type=int, default=200)\r\n",
        "        # IF params\r\n",
        "        parser.add_argument('--IF_n_trees', type=int, default=100)\r\n",
        "        parser.add_argument('--IF_sample_ratio', type=float, default=0.5)\r\n",
        "        # LOF params\r\n",
        "        parser.add_argument('--LOF_n_neighbors', type=int, default=20)\r\n",
        "        parser.add_argument('--LOF_n_leaf', type=int, default=30)\r\n",
        "        return parser\r\n",
        "\r\n",
        "\r\n",
        "def to_networkx_dataset(dataset):\r\n",
        "    return [to_networkx_featured(data) for data in dataset]\r\n",
        "\r\n",
        "def to_networkx_featured(data):\r\n",
        "    g = to_networkx(data, to_undirected=True, remove_self_loops=True)\r\n",
        "    features = {i: fea.argmax()+1 for i, fea in enumerate(data.x)}\r\n",
        "    nx.set_node_attributes(g, values=features, name='feature')\r\n",
        "    largest_cc = max(nx.connected_components(g), key=len)\r\n",
        "    largest_cc = g.subgraph(largest_cc)\r\n",
        "    # change node index\r\n",
        "    mapping = {old:new for new, old in enumerate(largest_cc.nodes)}\r\n",
        "    largest_cc = nx.relabel_nodes(largest_cc, mapping)\r\n",
        "    # option 1: remove the checking condition, and remove isolated nodes\r\n",
        "    # option 2: for every graph, add a new node, with node feature as 0\r\n",
        "    # and connect all nodes to this single virtual node\r\n",
        "    \r\n",
        "    return largest_cc # get the largest connected component"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQphRyjW7S0t"
      },
      "source": [
        "import torch\r\n",
        "import networkx as nx\r\n",
        "from torch_geometric.utils import to_networkx \r\n",
        "\r\n",
        "from karateclub import Graph2Vec, FGSD\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "from sklearn.neighbors import LocalOutlierFactor\r\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
        "\r\n",
        "\r\n",
        "from test_tube import HyperOptArgumentParser\r\n",
        "import logging\r\n",
        "\r\n",
        "\r\n",
        "class InductiveEmbeddingBasedGLAD:\r\n",
        "    def __init__(self, embedder, detector, \r\n",
        "                       G2V_nhid=64, G2V_wl_iter=2, \r\n",
        "                       FGSD_hist_bins=200, \r\n",
        "                       IF_n_trees=100, IF_sample_ratio=0.5,\r\n",
        "                       LOF_n_neighbors=20, LOF_n_leaf=30, **kwargs):\r\n",
        "        embedders = {\r\n",
        "            'Graph2Vec': Graph2Vec(wl_iterations=G2V_wl_iter, dimensions=G2V_nhid, \r\n",
        "                                    attributed=True, epochs=50),\r\n",
        "            'FGSD': FGSD(hist_bins=FGSD_hist_bins, hist_range=20)\r\n",
        "        }\r\n",
        "        detectors = {\r\n",
        "            'OCSVM': OneClassSVM(nu=0.001),\r\n",
        "            'LOF': LocalOutlierFactor(n_neighbors=LOF_n_neighbors, leaf_size=LOF_n_leaf, novelty=True, contamination=0.1)\r\n",
        "        }\r\n",
        "\r\n",
        "        assert embedder in embedders.keys()\r\n",
        "        assert detector in detectors.keys()\r\n",
        "\r\n",
        "        self.embedder = embedders[embedder]\r\n",
        "        self.detector = detectors[detector]\r\n",
        "        self.embedder_name = embedder\r\n",
        "        self.detector_name = detector\r\n",
        "\r\n",
        "    def __call__(self, train_dataset, test_dataset, raw=False):\r\n",
        "        # for inference, output anomaly score\r\n",
        "        if not raw:\r\n",
        "            train_dataset = to_networkx_dataset(train_dataset)\r\n",
        "            self.embedder.fit(train_dataset)\r\n",
        "            train_graph_embeddings = self.embedder.get_embedding()\r\n",
        "            \r\n",
        "            test_dataset = to_networkx_dataset(test_dataset)\r\n",
        "            self.embedder.fit(test_dataset)\r\n",
        "            test_graph_embeddings = self.embedder.get_embedding()\r\n",
        "            \r\n",
        "            self.detector.fit(train_graph_embeddings)\r\n",
        "\r\n",
        "        if self.detector_name == 'OCSVM':\r\n",
        "            anomaly_scores = -self.detector.decision_function(test_graph_embeddings)\r\n",
        "        else:\r\n",
        "            anomaly_scores = -self.detector.decision_function(test_graph_embeddings)\r\n",
        "\r\n",
        "        return anomaly_scores\r\n",
        "\r\n",
        "    def fit(self, train_dataset, test_dataset):\r\n",
        "        test_ys = torch.cat([data.y for data in test_dataset])\r\n",
        "        test_anomaly_scores = self(train_dataset, test_dataset)\r\n",
        "\r\n",
        "        roc_auc = roc_auc_score(test_ys, test_anomaly_scores)\r\n",
        "        pr_auc = average_precision_score(test_ys, test_anomaly_scores)\r\n",
        "        avg_score_normal = test_anomaly_scores[test_ys==0].mean()\r\n",
        "        avg_score_abnormal = test_anomaly_scores[test_ys==1].mean()\r\n",
        "\r\n",
        "        metrics = {'roc_auc': roc_auc,\r\n",
        "                'pr_auc': pr_auc,\r\n",
        "                'average_score_normal': avg_score_normal,\r\n",
        "                'average_score_anomaly': avg_score_abnormal}\r\n",
        "        return metrics\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def add_model_specific_args(parent_parser):\r\n",
        "        # nfeat should be inferred from data later\r\n",
        "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser], add_help=False)\r\n",
        "        # model \r\n",
        "        parser.add_argument('--embedder', type=str, default='Graph2Vec')\r\n",
        "        parser.add_argument('--detector', type=str, default='LOF')\r\n",
        "        # Graph2Vec params\r\n",
        "        parser.add_argument('--G2V_nhid', type=int, default=128)\r\n",
        "        parser.add_argument('--G2V_wl_iter', type=int, default=2)\r\n",
        "        # FSGD params\r\n",
        "        parser.add_argument('--FGSD_hist_bins', type=int, default=200)\r\n",
        "        # IF params\r\n",
        "        parser.add_argument('--IF_n_trees', type=int, default=100)\r\n",
        "        parser.add_argument('--IF_sample_ratio', type=float, default=0.5)\r\n",
        "        # LOF params\r\n",
        "        parser.add_argument('--LOF_n_neighbors', type=int, default=20)\r\n",
        "        parser.add_argument('--LOF_n_leaf', type=int, default=30)\r\n",
        "        return parser\r\n",
        "\r\n",
        "\r\n",
        "def to_networkx_dataset(dataset):\r\n",
        "    return [to_networkx_featured(data) for data in dataset]\r\n",
        "\r\n",
        "def to_networkx_featured(data):\r\n",
        "    g = to_networkx(data, to_undirected=True, remove_self_loops=True)\r\n",
        "    features = {i: fea.argmax()+1 for i, fea in enumerate(data.x)}\r\n",
        "    nx.set_node_attributes(g, values=features, name='feature')\r\n",
        "    largest_cc = max(nx.connected_components(g), key=len)\r\n",
        "    largest_cc = g.subgraph(largest_cc)\r\n",
        "    # change node index\r\n",
        "    mapping = {old:new for new, old in enumerate(largest_cc.nodes)}\r\n",
        "    largest_cc = nx.relabel_nodes(largest_cc, mapping)\r\n",
        "    # option 1: remove the checking condition, and remove isolated nodes\r\n",
        "    # option 2: for every graph, add a new node, with node feature as 0\r\n",
        "    # and connect all nodes to this single virtual node\r\n",
        "    \r\n",
        "    return largest_cc # get the largest connected component"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ybwhbC8fOuf"
      },
      "source": [
        "import torch, numpy as np\r\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram, Propagation, ShortestPath, PropagationAttr\r\n",
        "from sklearn.svm import OneClassSVM\r\n",
        "from sklearn.neighbors import LocalOutlierFactor\r\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
        "from test_tube import HyperOptArgumentParser\r\n",
        "\r\n",
        "class KernelBasedGLAD:\r\n",
        "    def __init__(self, kernel, detector, labeled=True,\r\n",
        "                       WL_iter=5, PK_bin_width=1,\r\n",
        "                       LOF_n_neighbors=20, LOF_n_leaf=30, **kwargs):\r\n",
        "        kernels = {\r\n",
        "            'WL': WeisfeilerLehman(n_iter=WL_iter, normalize=True, base_graph_kernel=VertexHistogram),\r\n",
        "            'PK': Propagation(t_max=WL_iter, w=PK_bin_width, normalize=True) if labeled else \r\n",
        "                  PropagationAttr(t_max=WL_iter, w=PK_bin_width, normalize=True),\r\n",
        "        }\r\n",
        "        detectors = {\r\n",
        "            'OCSVM': OneClassSVM(kernel='precomputed', nu=0.1),\r\n",
        "            'LOF': LocalOutlierFactor(n_neighbors=LOF_n_neighbors, leaf_size=LOF_n_leaf, \r\n",
        "                                      metric='precomputed', contamination=0.1)\r\n",
        "        }\r\n",
        "\r\n",
        "        assert kernel in kernels.keys()\r\n",
        "        assert detector in detectors.keys()\r\n",
        "        \r\n",
        "        self.kernel = kernels[kernel]\r\n",
        "        self.detector = detectors[detector]\r\n",
        "        self.kernel_name = kernel\r\n",
        "        self.detector_name = detector\r\n",
        "        self.labeled = labeled\r\n",
        "\r\n",
        "    def __call__(self, dataset):\r\n",
        "        # for inference, output anomaly score\r\n",
        "        dataset = to_grakel_dataset(dataset, self.labeled)\r\n",
        "        kernel_matrix = self.kernel.fit_transform(dataset)\r\n",
        "        if self.detector_name == 'OCSVM':\r\n",
        "            self.detector.fit(kernel_matrix)\r\n",
        "            anomaly_scores = -self.detector.decision_function(kernel_matrix)\r\n",
        "        else:\r\n",
        "            self.detector.fit(np.amax(kernel_matrix) - kernel_matrix)\r\n",
        "            anomaly_scores = -self.detector.negative_outlier_factor_\r\n",
        "\r\n",
        "        return anomaly_scores\r\n",
        "\r\n",
        "    def fit(self, dataset):\r\n",
        "        ys = torch.cat([data.y for data in dataset])\r\n",
        "        anomaly_scores = self(dataset)\r\n",
        "\r\n",
        "        roc_auc = roc_auc_score(ys, anomaly_scores)\r\n",
        "        pr_auc = average_precision_score(ys, anomaly_scores)\r\n",
        "        avg_score_normal = anomaly_scores[ys==0].mean()\r\n",
        "        avg_score_abnormal = anomaly_scores[ys==1].mean()  \r\n",
        "\r\n",
        "        metrics = {'roc_auc': roc_auc, \r\n",
        "                   'pr_auc': pr_auc, \r\n",
        "                   'average_score_normal': avg_score_normal,\r\n",
        "                   'average_score_anomaly': avg_score_abnormal}\r\n",
        "        return metrics\r\n",
        "    \r\n",
        "    def _get_kernel_matrices(self, normalize=True):\r\n",
        "        return calculate_kernel_matrix(self.kernel, normalize=normalize)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def add_model_specific_args(parent_parser):\r\n",
        "        # nfeat should be inferred from data later\r\n",
        "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser], add_help=False)\r\n",
        "        # model \r\n",
        "        parser.add_argument('--kernel', type=str, default='WL')\r\n",
        "        parser.add_argument('--detector', type=str, default='OCSVM')\r\n",
        "        parser.add_argument('--labeled', type=lambda x:x=='True', default=True)\r\n",
        "        # WL and PK params\r\n",
        "        parser.add_argument('--WL_iter', type=int, default=5)\r\n",
        "        parser.add_argument('--PK_bin_width', type=int, default=1)\r\n",
        "        # LOF params\r\n",
        "        parser.add_argument('--LOF_n_neighbors', type=int, default=20)\r\n",
        "        parser.add_argument('--LOF_n_leaf', type=int, default=30)\r\n",
        "        return parser\r\n",
        "\r\n",
        "def to_grakel_dataset(dataset, labeled=True):\r\n",
        "    def to_grakel_graph(data, labeled=True):\r\n",
        "        edges = {tuple(edge) for edge in data.edge_index.T.numpy()}\r\n",
        "        if labeled:\r\n",
        "            labels = {i: fea.argmax().item()+1 for i, fea in enumerate(data.x)}\r\n",
        "        else:\r\n",
        "            labels = {i: fea.numpy() for i, fea in enumerate(data.x)}\r\n",
        "        return [edges, labels] \r\n",
        "    return [to_grakel_graph(data, labeled) for data in dataset]\r\n",
        "    \r\n",
        "def calculate_kernel_matrix(kernel, normalize=True):\r\n",
        "    def pairwise_operation(x, y, kernel):\r\n",
        "        return np.array([kernel.metric(x[t], y[t]) for t in range(kernel.t_max)])\r\n",
        "    \r\n",
        "    X = kernel.X\r\n",
        "    kernel_matrices = np.zeros(shape=(kernel.t_max, len(X), len(X)))\r\n",
        "    cache = list()\r\n",
        "    for (i, x) in enumerate(X):\r\n",
        "        kernel_matrices[:,i,i] = pairwise_operation(x, x, kernel)\r\n",
        "        for (j, y) in enumerate(cache):\r\n",
        "            kernel_matrices[:,j,i]= pairwise_operation(y, x, kernel)\r\n",
        "        cache.append(x) \r\n",
        "    for i in range(kernel.t_max):\r\n",
        "        kernel_matrices[i] = np.triu(kernel_matrices[i]) + np.triu(kernel_matrices[i], 1).T \r\n",
        "        \r\n",
        "    accumulative_kernel_matrices = np.add.accumulate(kernel_matrices, 0)\r\n",
        "      \r\n",
        "    if normalize:\r\n",
        "        for i in range(kernel.t_max):\r\n",
        "            _X_diag = np.diagonal(kernel_matrices[i])\r\n",
        "            kernel_matrices[i] = kernel_matrices[i] / np.sqrt(np.outer(_X_diag, _X_diag))\r\n",
        "            \r\n",
        "            _X_diag = np.diagonal(accumulative_kernel_matrices[i])\r\n",
        "            accumulative_kernel_matrices[i] = accumulative_kernel_matrices[i] / np.sqrt(np.outer(_X_diag, _X_diag))\r\n",
        "            \r\n",
        "    return kernel_matrices, accumulative_kernel_matrices"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH0wDb50Qv8Z"
      },
      "source": [
        "import torch, numpy as np\r\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram, Propagation, ShortestPath, PropagationAttr\r\n",
        "from sklearn.svm import OneClassSVM\r\n",
        "from sklearn.neighbors import LocalOutlierFactor\r\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
        "from test_tube import HyperOptArgumentParser\r\n",
        "\r\n",
        "class InductiveKernelBasedGLAD:\r\n",
        "    def __init__(self, kernel, detector, labeled=True,\r\n",
        "                WL_iter=5, PK_bin_width=1,\r\n",
        "                LOF_n_neighbors=20, LOF_n_leaf=30, **kwargs):\r\n",
        "        # not sure how to set nu and contamination, especially for ocsvm\r\n",
        "        kernels = {\r\n",
        "              'WL': WeisfeilerLehman(n_iter=WL_iter, normalize=True, base_graph_kernel=VertexHistogram),\r\n",
        "              'PK': Propagation(t_max=WL_iter, w=PK_bin_width, normalize=True) if labeled else\r\n",
        "              PropagationAttr(t_max=WL_iter, w=PK_bin_width, normalize=True),\r\n",
        "              }\r\n",
        "        detectors = {\r\n",
        "              'OCSVM': OneClassSVM(kernel='precomputed', nu=0.001),\r\n",
        "              'LOF': LocalOutlierFactor(n_neighbors=LOF_n_neighbors, leaf_size=LOF_n_leaf,\r\n",
        "              metric='precomputed', novelty=True, contamination=0.1)\r\n",
        "              }\r\n",
        "\r\n",
        "        assert kernel in kernels.keys()\r\n",
        "        assert detector in detectors.keys()\r\n",
        "\r\n",
        "        self.kernel = kernels[kernel]\r\n",
        "        self.detector = detectors[detector]\r\n",
        "        self.kernel_name = kernel\r\n",
        "        self.detector_name = detector\r\n",
        "        self.labeled = labeled\r\n",
        "\r\n",
        "    def __call__(self, train_dataset, test_dataset, raw=False):\r\n",
        "        # for inference, output anomaly score\r\n",
        "        if not raw:\r\n",
        "            train_dataset = to_grakel_dataset(train_dataset, self.labeled)\r\n",
        "            test_dataset = to_grakel_dataset(test_dataset, self.labeled)\r\n",
        "            # get full dataset and kernel matrix\r\n",
        "            n_train, n_test = len(train_dataset), len(test_dataset)\r\n",
        "            dataset = train_dataset + test_dataset\r\n",
        "            kernel_matrix = self.kernel.fit_transform(dataset)\r\n",
        "            train_kernel_matrix = kernel_matrix[:n_train,:][:,:n_train]\r\n",
        "            test_kernel_matrix = kernel_matrix[n_train:,:][:,:n_train]\r\n",
        "\r\n",
        "        if self.detector_name == 'OCSVM':\r\n",
        "            self.detector.fit(train_kernel_matrix)\r\n",
        "            anomaly_scores = -self.detector.decision_function(test_kernel_matrix)\r\n",
        "        else:\r\n",
        "            self.detector.fit(np.amax(train_kernel_matrix) - train_kernel_matrix)\r\n",
        "            anomaly_scores = -self.detector.decision_function(test_kernel_matrix)\r\n",
        "\r\n",
        "        return anomaly_scores\r\n",
        "\r\n",
        "    def fit(self, train_dataset, test_dataset):\r\n",
        "        test_ys = torch.cat([data.y for data in test_dataset])\r\n",
        "        test_anomaly_scores = self(train_dataset, test_dataset)\r\n",
        "\r\n",
        "        roc_auc = roc_auc_score(test_ys, test_anomaly_scores)\r\n",
        "        pr_auc = average_precision_score(test_ys, test_anomaly_scores)\r\n",
        "        avg_score_normal = test_anomaly_scores[test_ys==0].mean()\r\n",
        "        avg_score_abnormal = test_anomaly_scores[test_ys==1].mean()\r\n",
        "\r\n",
        "        metrics = {'roc_auc': roc_auc,\r\n",
        "                'pr_auc': pr_auc,\r\n",
        "                'average_score_normal': avg_score_normal,\r\n",
        "                'average_score_anomaly': avg_score_abnormal}\r\n",
        "        return metrics\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def add_model_specific_args(parent_parser):\r\n",
        "        # nfeat should be inferred from data later\r\n",
        "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser], add_help=False)\r\n",
        "        # model\r\n",
        "        parser.add_argument('--kernel', type=str, default='WL')\r\n",
        "        parser.add_argument('--detector', type=str, default='LOF')\r\n",
        "        parser.add_argument('--labeled', type=lambda x:x=='True', default=True)\r\n",
        "        # WL and PK params\r\n",
        "        parser.add_argument('--WL_iter', type=int, default=5)\r\n",
        "        parser.add_argument('--PK_bin_width', type=int, default=1)\r\n",
        "        # LOF params\r\n",
        "        parser.add_argument('--LOF_n_neighbors', type=int, default=20)\r\n",
        "        parser.add_argument('--LOF_n_leaf', type=int, default=30)\r\n",
        "        return parser"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzDelFHZLog"
      },
      "source": [
        "import sys\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "def run_experiment_kernel(down_cls=0, down_rate=0.05, data_seed=1213, landmark_seed=100, data = \"mixhop\", kernel=\"WL\", detector=\"LOF\", layer=1):\n",
        "\n",
        "    \n",
        "    # load data\n",
        "    train_loader, test_loader, landmark_loader, num_features = create_loaders(data_name=data, \n",
        "                            batch_size=1, \n",
        "                            down_class=down_cls, \n",
        "                            down_rate=down_rate, \n",
        "                            dense=False,\n",
        "                            data_seed=data_seed,\n",
        "                            landmark_seed=0)\n",
        "\n",
        "\n",
        "    if detector=='OCSVM':\n",
        "        model = InductiveKernelBasedGLAD(kernel=kernel, detector=detector, WL_iter=layer)\n",
        "        results = model.fit(train_loader, test_loader)\n",
        "    \n",
        "    elif kernel=='LOF':\n",
        "        model = KernelBasedGLAD(kernel=kernel, detector=detector, WL_iter=layer)\n",
        "        results = model.fit(test_loader)\n",
        "\n",
        "    return results\n",
        "      \n",
        "def run_experiment_embedding(down_cls=0, down_rate=0.05, data_seed=1213, landmark_seed=100, data = \"mixhop\", embedder=\"Graph2Vec\", detector=\"LOF\", layer=1):\n",
        "\n",
        "    \n",
        "    # load data\n",
        "    train_loader, test_loader, landmark_loader, num_features = create_loaders(data_name=data, \n",
        "                            batch_size=1, \n",
        "                            down_class=down_cls, \n",
        "                            down_rate=down_rate, \n",
        "                            dense=False,\n",
        "                            data_seed=data_seed,\n",
        "                            landmark_seed=0)\n",
        "    \n",
        "    if detector=='OCSVM':\n",
        "        model = InductiveEmbeddingBasedGLAD(embedder=embedder, detector=detector, G2V_wl_iter=layer, FGSD_hist_bins=layer)\n",
        "        results = model.fit(train_loader, test_loader)\n",
        "    \n",
        "    elif kernel=='LOF':\n",
        "        model = EmbeddingBasedGLAD(embedder=embedder, detector=detector, G2V_wl_iter=layer, FGSD_hist_bins=layer)\n",
        "        results = model.fit(test_loader)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    return results\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "mGjXIyLFkJPz",
        "outputId": "4cc6524c-acaf-4b22-9c22-5c9983bb467a"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "kernel = \"WL\"\r\n",
        "detector = \"LOF\"\r\n",
        "print(\"Kernel=\", kernel)\r\n",
        "print(\"Detector=\", detector)\r\n",
        "\r\n",
        "for (data, down_cls) in [(\"mixhop\", 0), (\"PROTEINS\", 0), (\"Tox21_AhR_training\", 1), (\"AIDS\", 0)]:\r\n",
        "    print(data)\r\n",
        "    all_APs = []\r\n",
        "    all_ROC_AUCs = []\r\n",
        "    for layer in [1,2,4,8,16]:\r\n",
        "        print(\".\", end='')\r\n",
        "        res = run_experiment_kernel(down_cls=down_cls, data=data, kernel=kernel, detector=detector, layer=layer)\r\n",
        "\r\n",
        "        #print(\"layers=\", layer)\r\n",
        "        #print(\"AP=\", res['test_pr_auc'])\r\n",
        "        #print(\"ROC-AUC=\", res['test_roc_auc'])\r\n",
        "        all_APs.append(res['pr_auc'])\r\n",
        "        all_ROC_AUCs.append(res['roc_auc'])\r\n",
        "    \r\n",
        "    print(\"Average AP = %.2f +- %.2f, Average ROC-AUC = %.2f +- %.2f\" % (np.mean(all_APs), np.std(all_APs), np.mean(all_ROC_AUCs), np.std(all_ROC_AUCs)))\r\n",
        "\r\n",
        "kernel = \"WL\"\r\n",
        "detector = \"OCSVM\"\r\n",
        "print(\"Kernel=\", kernel)\r\n",
        "print(\"Detector=\", detector)\r\n",
        "\r\n",
        "for (data, down_cls) in [(\"mixhop\", 0), (\"PROTEINS\", 0), (\"Tox21_AhR_training\", 1), (\"AIDS\", 0)]:\r\n",
        "    print(data)\r\n",
        "    all_APs = []\r\n",
        "    all_ROC_AUCs = []\r\n",
        "    for layer in [1,2,4,8,16]:\r\n",
        "        print(\".\", end='')\r\n",
        "        res = run_experiment_kernel(down_cls=down_cls, data=data, kernel=kernel, detector=detector, layer=layer)\r\n",
        "\r\n",
        "        #print(\"layers=\", layer)\r\n",
        "        #print(\"AP=\", res['test_pr_auc'])\r\n",
        "        #print(\"ROC-AUC=\", res['test_roc_auc'])\r\n",
        "        all_APs.append(res['pr_auc'])\r\n",
        "        all_ROC_AUCs.append(res['roc_auc'])\r\n",
        "    \r\n",
        "    print(\"Average AP = %.2f +- %.2f, Average ROC-AUC = %.2f +- %.2f\" % (np.mean(all_APs), np.std(all_APs), np.mean(all_ROC_AUCs), np.std(all_ROC_AUCs)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kernel= WL\n",
            "Detector= LOF\n",
            "mixhop\n",
            ".....Average AP = 0.25 +- 0.17, Average ROC-AUC = 0.76 +- 0.08\n",
            "PROTEINS\n",
            ".....Average AP = 0.28 +- 0.07, Average ROC-AUC = 0.70 +- 0.03\n",
            "Tox21_AhR_training\n",
            ".....Average AP = 0.24 +- 0.04, Average ROC-AUC = 0.67 +- 0.05\n",
            "AIDS\n",
            ".....Average AP = 0.56 +- 0.05, Average ROC-AUC = 0.96 +- 0.01\n",
            "Kernel= WL\n",
            "Detector= OCSVM\n",
            "mixhop\n",
            "..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-595ede42ca97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdown_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(\"layers=\", layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-58f0dba5185b>\u001b[0m in \u001b[0;36mrun_experiment_kernel\u001b[0;34m(down_cls, down_rate, data_seed, landmark_seed, data, kernel, detector, layer)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelBasedGLAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWL_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#model = InductiveKernelBasedGLAD(kernel=kernel, detector=detector, WL_iter=layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c0af9cb006f9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0manomaly_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c0af9cb006f9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# for inference, output anomaly score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_grakel_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mkernel_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'OCSVM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c0af9cb006f9>\u001b[0m in \u001b[0;36mto_grakel_dataset\u001b[0;34m(dataset, labeled)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_grakel_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_kernel_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c0af9cb006f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_grakel_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_kernel_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c0af9cb006f9>\u001b[0m in \u001b[0;36mto_grakel_graph\u001b[0;34m(data, labeled)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0medge\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "JWSBw06ovvQh",
        "outputId": "b66171dc-ed00-46c2-f6fe-63fb777afbeb"
      },
      "source": [
        "import numpy as np\r\n",
        "'''\r\n",
        "embedder = \"Graph2Vec\"\r\n",
        "detector = \"LOF\"\r\n",
        "print(\"Embedder=\", embedder)\r\n",
        "print(\"Detector=\", detector)\r\n",
        "\r\n",
        "for (data, down_cls) in [(\"mixhop\", 0), (\"PROTEINS\", 0), (\"Tox21_AhR_training\", 1), (\"AIDS\", 0)]:\r\n",
        "    print(data)\r\n",
        "    all_APs = []\r\n",
        "    all_ROC_AUCs = []\r\n",
        "    for layer in [1,2,4,8,16]:\r\n",
        "        print(\".\", end='')\r\n",
        "        res = run_experiment_embedding(down_cls=down_cls, data=data, embedder=embedder, detector=detector, layer=layer)\r\n",
        "\r\n",
        "        #print(\"layers=\", layer)\r\n",
        "        #print(\"AP=\", res['test_pr_auc'])\r\n",
        "        #print(\"ROC-AUC=\", res['test_roc_auc'])\r\n",
        "        all_APs.append(res['pr_auc'])\r\n",
        "        all_ROC_AUCs.append(res['roc_auc'])\r\n",
        "    \r\n",
        "    print(\"Average AP = %.2f +- %.2f, Average ROC-AUC = %.2f +- %.2f\" % (np.mean(all_APs), np.std(all_APs), np.mean(all_ROC_AUCs), np.std(all_ROC_AUCs)))\r\n",
        "'''\r\n",
        "embedder = \"Graph2Vec\"\r\n",
        "detector = \"OCSVM\"\r\n",
        "print(\"Embedder=\", embedder)\r\n",
        "print(\"Detector=\", detector)\r\n",
        "\r\n",
        "for (data, down_cls) in [(\"AIDS\", 0), (\"PROTEINS\", 0), (\"Tox21_AhR_training\", 1), (\"AIDS\", 0)]:\r\n",
        "    print(data)\r\n",
        "    all_APs = []\r\n",
        "    all_ROC_AUCs = []\r\n",
        "    for layer in [1,2,4,8,16]:\r\n",
        "        print(\".\", end='')\r\n",
        "        res = run_experiment_embedding(down_cls=down_cls, data=data, embedder=embedder, detector=detector, layer=layer)\r\n",
        "\r\n",
        "        #print(\"layers=\", layer)\r\n",
        "        #print(\"AP=\", res['test_pr_auc'])\r\n",
        "        #print(\"ROC-AUC=\", res['test_roc_auc'])\r\n",
        "        all_APs.append(res['pr_auc'])\r\n",
        "        all_ROC_AUCs.append(res['roc_auc'])\r\n",
        "    \r\n",
        "    print(\"Average AP = %.2f +- %.2f, Average ROC-AUC = %.2f +- %.2f\" % (np.mean(all_APs), np.std(all_APs), np.mean(all_ROC_AUCs), np.std(all_ROC_AUCs)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedder= Graph2Vec\n",
            "Detector= OCSVM\n",
            "AIDS\n",
            ".After downsampling and test-train splitting, distribution of classes:\n",
            "TRAIN: Number of graphs: 202, Class distribution ['202', '0']\n",
            "TEST: Number of graphs: 206, Class distribution ['198', '8']\n",
            "Number of node features: 38\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:315: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
            "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-854f297e9cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdown_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(\"layers=\", layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-71183101aec1>\u001b[0m in \u001b[0;36mrun_experiment_embedding\u001b[0;34m(down_cls, down_rate, data_seed, landmark_seed, data, embedder, detector, layer)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'OCSVM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInductiveEmbeddingBasedGLAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG2V_wl_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFGSD_hist_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'LOF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtest_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtest_anomaly_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_anomaly_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, train_dataset, test_dataset, raw)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtrain_graph_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_networkx_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtest_graph_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36mto_networkx_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36mto_networkx_featured\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_undirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_self_loops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_node_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mlargest_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f67110357d5d>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_networkx_featured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_undirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_self_loops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_node_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mlargest_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4vn5feStdtu"
      },
      "source": [
        "for (data, down_cls) in [(\"saved\", 0), (\"PROTEINS\", 0), (\"NCI1\", 1)]:\r\n",
        "    for embedder in ['Graph2Vec', 'FGSD']:\r\n",
        "        if embedder == 'Graph2Vec':\r\n",
        "            layers = [1,2,4,8,16]\r\n",
        "        else:\r\n",
        "            layers = [25,50,100,200,400]\r\n",
        "        for detector in ['LOF', 'IF']:\r\n",
        "            for layer in layers:\r\n",
        "                res = run_experiment_embedding(down_cls=down_cls, data=data, embedder=embedder, detector=detector, layer=layer)\r\n",
        "\r\n",
        "                print(data)\r\n",
        "                print(\"Embedder=\", embedder)\r\n",
        "                print(\"Detector=\", detector)\r\n",
        "                print(\"layers=\", layer)\r\n",
        "                print(\"AP=\", res['pr_auc'])\r\n",
        "                print(\"ROC-AUC=\", res['roc_auc'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}