{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_selection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOKyAWHw8v7j"
      },
      "source": [
        "import numpy as np\r\n",
        "from scipy.stats import spearmanr, kendalltau\r\n",
        "from sklearn.metrics import ndcg_score\r\n",
        "\r\n",
        "def reliability_scores(D, narrow=False, measure=\"spearman\", aggregation=\"mean\", preprocess=\"none\", idx='external'):\r\n",
        "    keys = D.keys()\r\n",
        "    total_models = len(keys)\r\n",
        "\r\n",
        "    if narrow:\r\n",
        "        distinct_models = list(set([(k[0], k[1], k[-1]) for k in keys]))\r\n",
        "        runs_per_model = total_models // len(distinct_models)\r\n",
        "        similarity_matrix = np.zeros((total_models, runs_per_model-1))\r\n",
        "    else:\r\n",
        "        similarity_matrix = np.zeros((total_models, total_models-1))\r\n",
        "\r\n",
        "    if preprocess==\"maxnormalize\" or \"minmaxnormalize\":\r\n",
        "        all_dists = np.array([])\r\n",
        "        for k in keys:\r\n",
        "            all_dists = np.append(all_dists, D[k][idx].dists.numpy())\r\n",
        "\r\n",
        "        maxdist = max(all_dists)\r\n",
        "        mindist = min(all_dists)\r\n",
        "\r\n",
        "    for i,k in enumerate(keys):\r\n",
        "        kk = (k[0], k[1], k[-1])\r\n",
        "        dists_k = D[k][idx].dists.numpy()\r\n",
        "\r\n",
        "        if preprocess==\"maxnormalize\":\r\n",
        "            dists_k  = dists_k/maxdist\r\n",
        "        elif preprocess==\"minmaxnormalize\":\r\n",
        "            dists_k = (dists_k - mindist)/(maxdist-mindist)\r\n",
        "        elif preprocess==\"rank\":\r\n",
        "            temp = (-dists_k).argsort()\r\n",
        "            ranks = np.empty_like(temp)\r\n",
        "            ranks[temp] = np.arange(1,len(dists_k)+1)\r\n",
        "            dists_k = 1/ranks\r\n",
        "        \r\n",
        "        if narrow:\r\n",
        "            other_runs = [j for j in keys if j != k and (j[0], j[1], j[-1]) == kk]\r\n",
        "        \r\n",
        "        else:\r\n",
        "            other_runs = [j for j in keys if j != k]\r\n",
        "        \r\n",
        "        for j,l in enumerate(other_runs):\r\n",
        "            dists_l = D[l][idx].dists.numpy()\r\n",
        "\r\n",
        "            if preprocess==\"maxnormalize\":\r\n",
        "                dists_l  = dists_l/maxdist\r\n",
        "            elif preprocess==\"minmaxnormalize\":\r\n",
        "                dists_l = (dists_l - mindist)/(maxdist-mindist)\r\n",
        "            elif preprocess==\"rank\":\r\n",
        "                temp = (-dists_l).argsort()\r\n",
        "                ranks = np.empty_like(temp)\r\n",
        "                ranks[temp] = np.arange(1,len(dists_l)+1)\r\n",
        "                dists_l = 1/ranks\r\n",
        "        \r\n",
        "            if measure == \"spearman\":\r\n",
        "                score = spearmanr(dists_k, dists_l)[0]\r\n",
        "            elif measure == \"KT\":\r\n",
        "                score = kendalltau(dists_k, dists_l)[0]\r\n",
        "            elif measure == \"NDCG\":\r\n",
        "                score = (ndcg_score(np.asarray([dists_k]), np.asarray([dists_l])) + ndcg_score(np.asarray([dists_k]), np.asarray([dists_l])))/2\r\n",
        "            else:\r\n",
        "                print(\"wrong measure\")\r\n",
        "                return -1\r\n",
        "            \r\n",
        "            similarity_matrix[i,j] = score\r\n",
        "            \r\n",
        "    if aggregation == \"mean\":\r\n",
        "        reliability_scores = np.mean(similarity_matrix, 1)\r\n",
        "    elif aggregation == \"median\":\r\n",
        "        reliability_scores = np.median(similarity_matrix, 1)\r\n",
        "    else:\r\n",
        "        print(\"wrong aggregation\")\r\n",
        "        return -1\r\n",
        "\r\n",
        "    return reliability_scores"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uECBBm7inHo1"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def HITS(D, nu = 0.05, iters=10, init = \"rank\", idx='external'):\r\n",
        "    keys = D.keys()\r\n",
        "    os_lists = [D[k][idx].dists for k in keys]\r\n",
        "    no_of_models = len(os_lists)\r\n",
        "    no_of_points = len(os_lists[0])\r\n",
        "    \r\n",
        "    if init == \"soft\":\r\n",
        "        label_list = os_lists\r\n",
        "    elif init == \"hard\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            ind = np.argpartition(os_list, -int(nu*no_of_points))[-int(nu*no_of_points):]\r\n",
        "            labels = np.array([1 if idx in ind else 0 for idx in range(no_of_points)])\r\n",
        "            label_list.append(labels)\r\n",
        "    elif init == \"semisoft\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            ind = np.argpartition(os_list, -int(nu*no_of_points))[-int(nu*no_of_points):]\r\n",
        "            labels = np.array([os_list[idx] if idx in ind else 0 for idx in range(no_of_points)])\r\n",
        "            label_list.append(labels)\r\n",
        "    elif init == \"rank\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            temp = (-np.array(os_list)).argsort()\r\n",
        "            ranks = np.empty_like(temp)\r\n",
        "            ranks[temp] = np.arange(1,len(os_list)+1)\r\n",
        "            labels = (1/ranks).tolist()\r\n",
        "            label_list.append(labels)\r\n",
        "\r\n",
        "    model_weights = np.ones(no_of_models)\r\n",
        "\r\n",
        "    for _ in range(iters):\r\n",
        "\r\n",
        "        label_matrix = np.stack(label_list)\r\n",
        "        point_weights = np.matmul(model_weights, label_matrix)\r\n",
        "        point_weights = point_weights/np.linalg.norm(point_weights)\r\n",
        "        \r\n",
        "        model_weights = np.matmul(label_matrix, point_weights)\r\n",
        "        model_weights = model_weights/np.linalg.norm(model_weights)\r\n",
        "\r\n",
        "        best_model = np.argmax(model_weights)\r\n",
        "\r\n",
        "\r\n",
        "    return list(keys)[best_model], point_weights\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJaNvYBeeHb1"
      },
      "source": [
        "def dawidskeene(D, nu = 0.05, iters=10, init = \"soft\", idx=0):\r\n",
        "    print(\"init=\", init)\r\n",
        "    keys = D.keys()\r\n",
        "    os_lists = [D[k][idx].dists for k in keys]\r\n",
        "    no_of_models = len(os_lists)\r\n",
        "    no_of_points = len(os_lists[0])\r\n",
        "    \r\n",
        "    if init == \"soft\":\r\n",
        "        label_list = os_lists\r\n",
        "    elif init == \"hard\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            ind = np.argpartition(os_list, -int(nu*no_of_points))[-int(nu*no_of_points):]\r\n",
        "            labels = np.array([1 if idx in ind else 0 for idx in range(no_of_points)])\r\n",
        "            label_list.append(labels)\r\n",
        "    elif init == \"semisoft\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            ind = np.argpartition(os_list, -int(nu*no_of_points))[-int(nu*no_of_points):]\r\n",
        "            labels = np.array([os_list[idx] if idx in ind else 0 for idx in range(no_of_points)])\r\n",
        "            label_list.append(labels)\r\n",
        "    elif init == \"rank\":\r\n",
        "        label_list = []\r\n",
        "        for os_list in os_lists:\r\n",
        "            temp = (-np.array(os_list)).argsort()\r\n",
        "            ranks = np.empty_like(temp)\r\n",
        "            ranks[temp] = np.arange(1,len(os_list)+1)\r\n",
        "            labels = (1/ranks).tolist()\r\n",
        "            label_list.append(labels)\r\n",
        "    \r\n",
        "    \r\n",
        "    def E(model_weights, pi0=0.95):\r\n",
        "\r\n",
        "        pi1 = 1-pi0\r\n",
        "    \r\n",
        "        #eta1s = []\r\n",
        "        #eta0s = []\r\n",
        "        rho0s = []\r\n",
        "        for j in range(no_of_points):\r\n",
        "            #eta1 = np.product([model_weights[i] if label_list[i][j] == 1 else 1-model_weights[i] for i in range(no_of_models)])\r\n",
        "            eta1 = np.product([model_weights[i] if label_list[i][j] == 1 else 1 for i in range(no_of_models)])\r\n",
        "            #eta1s.append(eta1)\r\n",
        "            #eta0 = np.product([model_weights[i] if label_list[i][j] == 0 else 1-model_weights[i] for i in range(no_of_models)])\r\n",
        "            eta0 = np.product([model_weights[i] if label_list[i][j] == 0 else 1 for i in range(no_of_models)])\r\n",
        "            #eta0s.append(eta0)\r\n",
        "            rho0 = (pi0*eta0)/(pi0*eta0 + pi1*eta1)\r\n",
        "            rho0s.append(rho0)\r\n",
        "        \r\n",
        "        return rho0s\r\n",
        "    \r\n",
        "    def M(rho0s):\r\n",
        "\r\n",
        "        model_weights = []\r\n",
        "\r\n",
        "        for i in range(no_of_models):\r\n",
        "            mw = np.mean([rho0s[j] if label_list[i][j] == 0 else 1-rho0s[j] for j in range(no_of_points)])\r\n",
        "            model_weights.append(mw)\r\n",
        "\r\n",
        "        pi0 = np.mean(rho0s)\r\n",
        "\r\n",
        "        return model_weights, pi0\r\n",
        "\r\n",
        "\r\n",
        "    label_matrix = np.stack(label_list)\r\n",
        "    label_means = np.mean(label_matrix, axis=0)\r\n",
        "    majority_labels = label_means >= 0.5\r\n",
        "    \r\n",
        "    model_weights = [np.mean(labels == majority_labels) for labels in label_list]\r\n",
        "    \r\n",
        "    pi0 = 1- np.mean(majority_labels)\r\n",
        "\r\n",
        "    print(model_weights, pi0)\r\n",
        "    for _ in range(iters):\r\n",
        "        rho0s = E(model_weights, pi0)\r\n",
        "        #print(rho0s)\r\n",
        "        model_weights, pi0 = M(rho0s)\r\n",
        "        #print(model_weights, pi0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N9Mv8mO3nNk"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "with open('all_models_Tox21_AhR_training_1213.pkl', 'rb') as f:\r\n",
        "    D = pickle.load(f)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wjnm_mB8qrW",
        "outputId": "13c61f48-e5c3-49cd-80f7-3a92268d72f5"
      },
      "source": [
        "from sklearn.metrics import average_precision_score, roc_auc_score\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "for idx in ['external', 'converged', 'last']:\r\n",
        "    \r\n",
        "    if idx == 'external':\r\n",
        "        print(\"Epoch selection: CH2\")\r\n",
        "    elif idx == 'converged':\r\n",
        "        print(\"Epoch selection: convergence\")\r\n",
        "    elif idx == 'last':\r\n",
        "        print(\"Epoch selection: 150th epoch\")\r\n",
        "\r\n",
        "    all_aps = []\r\n",
        "    all_roc_aucs = []\r\n",
        "    min_svdd_model = None\r\n",
        "    min_svdd = np.inf\r\n",
        "    for k in D.keys():\r\n",
        "        model = D[k][idx]\r\n",
        "        all_aps.append(model.ap)\r\n",
        "        all_roc_aucs.append(model.roc_auc)\r\n",
        "        \r\n",
        "        if model.svdd_loss < min_svdd:\r\n",
        "            min_svdd_model = model\r\n",
        "            min_svdd = model.svdd_loss\r\n",
        "\r\n",
        "    print(\"\\tAverage: AP=%.2f +- %.2f, ROC-AUC=%.2f += %.2f\" % (np.mean(all_aps), np.std(all_aps), np.mean(all_roc_aucs), np.std(all_roc_aucs), ))\r\n",
        "    print(\"\\tAt Min SVDD: AP=%.2f, ROC-AUC=%.2f\" % (min_svdd_model.ap, min_svdd_model.roc_auc))\r\n",
        "    \r\n",
        "    '''\r\n",
        "    for narrow in [True, False]:\r\n",
        "        for aggregation in [\"mean\", \"median\"]:\r\n",
        "            for measure in [\"spearman\", \"KT\", \"NDCG\"]:\r\n",
        "                for preprocess in [\"none\", \"rank\", \"maxnormalize\", \"minmaxnormalize\"]:\r\n",
        "                    rel = reliability_scores(D, narrow=narrow, measure=measure, aggregation=aggregation, preprocess=preprocess, idx=idx)\r\n",
        "                    max_idx = np.argmax(rel)\r\n",
        "                    v = list(D.values())[max_idx]\r\n",
        "                    print(v[idx].ap, v[idx].roc_auc)\r\n",
        "                    print(\"\\n\\n\")\r\n",
        "    '''\r\n",
        "\r\n",
        "    rel = reliability_scores(D, narrow=False, measure=\"spearman\", aggregation=\"median\", preprocess=\"none\", idx=idx)\r\n",
        "    max_idx = np.argmax(rel)\r\n",
        "    v = list(D.values())[max_idx]\r\n",
        "    print(\"\\tMC: AP=%.2f, ROC-AUC=%.2f\" % (v[idx].ap, v[idx].roc_auc))\r\n",
        "\r\n",
        "    rel = reliability_scores(D, narrow=True, measure=\"spearman\", aggregation=\"mean\", preprocess=\"none\", idx=idx)\r\n",
        "    max_idx = np.argmax(rel)\r\n",
        "    v = list(D.values())[max_idx]\r\n",
        "    print(\"\\tUDR: AP=%.2f, ROC-AUC=%.2f\" % (v[idx].ap, v[idx].roc_auc))\r\n",
        "\r\n",
        "    for init in [\"soft\", \"rank\"]:\r\n",
        "        \r\n",
        "        if init==\"soft\":\r\n",
        "            print(\"\\tHITS using actual scores:\")\r\n",
        "        elif init==\"rank\":\r\n",
        "            print(\"\\tHITS using 1/rank:\")\r\n",
        "        best_hits_key, combined_hits_scores = HITS(D, init=init, idx=idx)\r\n",
        "        print(\"\\t\\tBest model: AP=%.2f, ROC-AUC=%.2f\" % (D[best_hits_key][idx].ap, D[best_hits_key][idx].roc_auc))\r\n",
        "\r\n",
        "        labels = D[best_hits_key][idx].labels\r\n",
        "        ap = average_precision_score(labels, combined_hits_scores)\r\n",
        "        roc_auc = roc_auc_score(labels, combined_hits_scores)\r\n",
        "        print(\"\\t\\tEnsemble model: AP=%.2f, ROC-AUC=%.2f\" % (ap, roc_auc))\r\n",
        "\r\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch selection: CH2\n",
            "\tAverage: AP=0.17 +- 0.03, ROC-AUC=0.70 += 0.04\n",
            "\tAt Min SVDD: AP=0.15, ROC-AUC=0.63\n",
            "\tMC: AP=0.17, ROC-AUC=0.67\n",
            "\tUDR: AP=0.14, ROC-AUC=0.65\n",
            "\tHITS using actual scores:\n",
            "\t\tBest model: AP=0.14, ROC-AUC=0.67\n",
            "\t\tEnsemble model: AP=0.18, ROC-AUC=0.77\n",
            "\tHITS using 1/rank:\n",
            "\t\tBest model: AP=0.18, ROC-AUC=0.70\n",
            "\t\tEnsemble model: AP=0.22, ROC-AUC=0.75\n",
            "\n",
            "\n",
            "\n",
            "Epoch selection: convergence\n",
            "\tAverage: AP=0.17 +- 0.03, ROC-AUC=0.69 += 0.03\n",
            "\tAt Min SVDD: AP=0.17, ROC-AUC=0.74\n",
            "\tMC: AP=0.19, ROC-AUC=0.71\n",
            "\tUDR: AP=0.15, ROC-AUC=0.68\n",
            "\tHITS using actual scores:\n",
            "\t\tBest model: AP=0.11, ROC-AUC=0.67\n",
            "\t\tEnsemble model: AP=0.13, ROC-AUC=0.68\n",
            "\tHITS using 1/rank:\n",
            "\t\tBest model: AP=0.18, ROC-AUC=0.69\n",
            "\t\tEnsemble model: AP=0.20, ROC-AUC=0.74\n",
            "\n",
            "\n",
            "\n",
            "Epoch selection: 150th epoch\n",
            "\tAverage: AP=0.17 +- 0.03, ROC-AUC=0.70 += 0.04\n",
            "\tAt Min SVDD: AP=0.17, ROC-AUC=0.74\n",
            "\tMC: AP=0.21, ROC-AUC=0.76\n",
            "\tUDR: AP=0.19, ROC-AUC=0.70\n",
            "\tHITS using actual scores:\n",
            "\t\tBest model: AP=0.17, ROC-AUC=0.72\n",
            "\t\tEnsemble model: AP=0.18, ROC-AUC=0.74\n",
            "\tHITS using 1/rank:\n",
            "\t\tBest model: AP=0.17, ROC-AUC=0.72\n",
            "\t\tEnsemble model: AP=0.20, ROC-AUC=0.78\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qftrVzcC66mN"
      },
      "source": [
        "def add_scatter_plot(D, ax, color, selected_indices=[\"external\"], cutoff = None, find_knee = False):\r\n",
        "\r\n",
        "\r\n",
        "    output_string = \"\"\r\n",
        "    all_APs = []\r\n",
        "    all_SVDDs = []\r\n",
        "\r\n",
        "    for v in D.values():\r\n",
        "        for selected_index in selected_indices:\r\n",
        "            epoch = v[selected_index]\r\n",
        "            all_APs.append(epoch.ap)\r\n",
        "            all_SVDDs.append(epoch.svdd_loss)\r\n",
        "\r\n",
        "    all_APs = sorted(list(set(all_APs)), reverse=True)\r\n",
        "    all_SVDDs = sorted(all_SVDDs)\r\n",
        "\r\n",
        "    if cutoff == None:\r\n",
        "        cutoff_SVDD = all_SVDDs[-1]\r\n",
        "    else:\r\n",
        "        cutoff_SVDD = all_SVDDs[int(cutoff*len(all_SVDDs))]\r\n",
        "\r\n",
        "    for selected_index in selected_indices:\r\n",
        "        if selected_index == \"external\":\r\n",
        "            output_string += \"\\nFor CH2-stop epochs with SVDD below (%.4f)\" % (cutoff_SVDD)\r\n",
        "        elif selected_index == \"converged\":\r\n",
        "            output_string += \"\\nFor conv-stop epochs with SVDD below (%.4f)\" % (cutoff_SVDD)\r\n",
        "        elif selected_index == \"last\":\r\n",
        "            output_string += \"\\nFor 150th epochs with SVDD below (%.4f)\" % (cutoff_SVDD)\r\n",
        "        \r\n",
        "    min_svdd = np.inf\r\n",
        "    total_AP = 0\r\n",
        "    total_ROC = 0\r\n",
        "\r\n",
        "    ch_epochs = []\r\n",
        "\r\n",
        "    AP_dict = {}\r\n",
        "    ROC_dict = {}\r\n",
        "    for v in D.values():\r\n",
        "        \r\n",
        "        for selected_index in selected_indices:\r\n",
        "            epoch = v[selected_index]\r\n",
        "\r\n",
        "            if epoch.svdd_loss < min_svdd:\r\n",
        "                min_svdd = epoch.svdd_loss\r\n",
        "                min_svdd_AP = epoch.ap\r\n",
        "                min_svdd_ROC = epoch.roc_auc\r\n",
        "            \r\n",
        "            if len(all_APs) < 10 or epoch.ap >= all_APs[9]:\r\n",
        "                ax2.scatter(epoch.svdd_loss, -epoch.ch2, s=50*epoch.ap*epoch.ap, color=color, alpha=0.4, linewidths=0, marker='s')\r\n",
        "                ax2.annotate(\"%0.2f\" % (epoch.ap), (epoch.svdd_loss, -epoch.ch2), fontsize=3, color=color, bbox=bbox_props)\r\n",
        "            else:\r\n",
        "                ax2.scatter(epoch.svdd_loss, -epoch.ch2, s=50*epoch.ap*epoch.ap, color=color, alpha=0.4, linewidths=0)\r\n",
        "\r\n",
        "            AP_dict[(epoch.svdd_loss, -epoch.ch2)] = epoch.ap\r\n",
        "            ROC_dict[(epoch.svdd_loss, -epoch.ch2)] = epoch.roc_auc\r\n",
        "\r\n",
        "            if epoch.svdd_loss <= cutoff_SVDD:\r\n",
        "                ch_epochs.append(epoch)\r\n",
        "                total_AP += epoch.ap\r\n",
        "                total_ROC += epoch.roc_auc\r\n",
        "\r\n",
        "        \r\n",
        "    output_string += \"\\nAP at min SVDD = %.2f\\naverage AP = %.2f\" % (min_svdd_AP, total_AP/len(ch_epochs))\r\n",
        "    output_string += \"\\nROCAUC at min SVDD = %.2f\\naverage ROCAUC = %.2f\" % (min_svdd_ROC, total_ROC/len(ch_epochs))\r\n",
        "\r\n",
        "    if find_knee:\r\n",
        "        Xs = [e.svdd_loss for e in ch_epochs]\r\n",
        "        Ys = [-e.ch2 for e in ch_epochs]\r\n",
        "\r\n",
        "        pf_X, pf_Y = find_pareto_frontier(Xs, Ys)\r\n",
        "        for x,y in zip(pf_X, pf_Y):\r\n",
        "            AP = AP_dict[(x,y)]\r\n",
        "            ROC = ROC_dict[(x,y)]\r\n",
        "            ax2.annotate(\"%0.2f (%0.2f)\" % (AP, ROC), (x, y), fontsize=4, color=color, bbox=bbox_props)\r\n",
        "        ax2.plot(pf_X, pf_Y, color=color, alpha=1)\r\n",
        "\r\n",
        "        from kneed import KneeLocator\r\n",
        "        kneedle = KneeLocator(pf_X, pf_Y, S=1.0, curve=\"convex\", direction=\"decreasing\")\r\n",
        "        try:\r\n",
        "            output_string += \"\\nKnee point: (%.4f, %.2f)\\nAP at knee = %.2f; ROCAUC at knee = %.2f\" % (kneedle.knee, kneedle.knee_y, AP_dict[(kneedle.knee, kneedle.knee_y)], ROC_dict[(kneedle.knee, kneedle.knee_y)])\r\n",
        "        except KeyError:\r\n",
        "            output_string += \"\\nKnee point not found\"\r\n",
        "\r\n",
        "\r\n",
        "    ch_distlist = [e.dists for e in ch_epochs]\r\n",
        "\r\n",
        "    '''\r\n",
        "    best_ndcg_index, best_ndcg = find_best_ndcg(ch_distlist, init=\"hard\")\r\n",
        "    best_ndcg_epoch = ch_epochs[best_ndcg_index]\r\n",
        "    ax2.scatter(best_ndcg_epoch.svdd_loss, -best_ndcg_epoch.ch, s=50, alpha=0.7, color='yellow', linewidths=0, marker='x')\r\n",
        "    ax2.annotate(\"%0.2f\" % (best_ndcg_epoch.ap), (best_ndcg_epoch.svdd_loss, -best_ndcg_epoch.ch), fontsize=4, color=color, bbox=bbox_props)\r\n",
        "    output_string += \"\\nMax NDCG at: (%.4f, %.2f)\\nNDCG= %.2f; AP= %.2f\" % (best_ndcg_epoch.svdd_loss, -best_ndcg_epoch.ch, best_ndcg, best_ndcg_epoch.ap)\r\n",
        "\r\n",
        "\r\n",
        "    best_kt_index, best_kt = find_best_kt(ch_distlist, init=\"semisoft\")\r\n",
        "    best_kt_epoch = ch_epochs[best_kt_index]\r\n",
        "    ax2.scatter(best_kt_epoch.svdd_loss, -best_kt_epoch.ch, s=50, alpha=0.7, color='yellow', linewidths=0, marker='x')\r\n",
        "    ax2.annotate(\"%0.2f\" % (best_kt_epoch.ap), (best_kt_epoch.svdd_loss, -best_kt_epoch.ch), fontsize=4, color=color, bbox=bbox_props)\r\n",
        "    output_string += \"\\nMax KT at: (%.4f, %.2f)\\nKT= %.2f; AP= %.2f\" % (best_kt_epoch.svdd_loss, -best_kt_epoch.ch, best_ndcg, best_kt_epoch.ap)\r\n",
        "    \r\n",
        "\r\n",
        "    best_hits_index, hits_combined_ap = HITS(ch_distlist, init=\"hard\")\r\n",
        "    best_hits_epoch = ch_epochs[best_hits_index]\r\n",
        "    ax2.scatter(best_hits_epoch.svdd_loss, -best_hits_epoch.ch2, s=50, alpha=0.7, color='yellow', linewidths=0, marker='x')\r\n",
        "    ax2.annotate(\"%0.2f\" % (best_hits_epoch.ap), (best_hits_epoch.svdd_loss, -best_hits_epoch.ch2), fontsize=4, color=color, bbox=bbox_props)\r\n",
        "    output_string += \"\\nAP of combined HITS model = %.2f\" % (hits_combined_ap)\r\n",
        "    output_string += \"\\nBest HITS model at: (%.4f, %.2f); AP= %.2f\" % (best_hits_epoch.svdd_loss, -best_hits_epoch.ch2, best_hits_epoch.ap)\r\n",
        "    '''\r\n",
        "    return output_string"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPKlzTDvi4mC"
      },
      "source": [
        "def find_pareto_frontier(Xs, Ys):\n",
        "    '''Pareto frontier selection process'''\n",
        "    sorted_list = sorted([[Xs[i], Ys[i]] for i in range(len(Xs))])\n",
        "    pareto_front = [sorted_list[0]]\n",
        "    for pair in sorted_list[1:]:\n",
        "        if pair[1] <= pareto_front[-1][1]:\n",
        "            pareto_front.append(pair)\n",
        "    \n",
        "    pf_X = [pair[0] for pair in pareto_front]\n",
        "    pf_Y = [pair[1] for pair in pareto_front]    \n",
        "    return pf_X, pf_Y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkw5W6RQCX0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027c4b48-8273-4619-9365-d7b95bb796a9"
      },
      "source": [
        "!pip install kneed"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kneed\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/6b/e130913aaaad1373060e259ab222ca2330672db696b297b082c3f3089fcc/kneed-0.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from kneed) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from kneed) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from kneed) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->kneed) (1.15.0)\n",
            "Installing collected packages: kneed\n",
            "Successfully installed kneed-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cje97xvLAGJe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6a96f088-a60a-4b6f-a743-81a129560105"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "import numpy as np\n",
        "\n",
        "colors = [\"blue\", \"black\", \"red\"]\n",
        "txtstrng = \"\"\n",
        "\n",
        "bbox_props = dict(boxstyle=\"round\", fc=\"w\", ec=\"none\", alpha=0.7)\n",
        "\n",
        "fig2, ax2 = plt.subplots()\n",
        "lines = [Line2D([0], [0], color=c) for c in colors]\n",
        "labels = []\n",
        "ax2.set(xlabel='SVDD loss', ylabel='-CH')\n",
        "ax2.title.set_text('All points')\n",
        "\n",
        "label = \"at min CH value\"\n",
        "labels.append(label)\n",
        "label = \"at convergence\"\n",
        "labels.append(label)\n",
        "label = \"at epoch 150\"\n",
        "labels.append(label)\n",
        "\n",
        "txtstrng += add_scatter_plot(D, ax2, colors[0], selected_indices=[\"external\"], find_knee=True)\n",
        "txtstrng += \"\\n\\n\"\n",
        "txtstrng += add_scatter_plot(D, ax2, colors[1], selected_indices=[\"converged\"], find_knee=False)\n",
        "txtstrng += \"\\n\\n\"\n",
        "txtstrng += add_scatter_plot(D, ax2, colors[2], selected_indices=[\"last\"], find_knee=False)\n",
        "\n",
        "print(txtstrng)\n",
        "\n",
        "ax2.text(0.7, 0.8, txtstrng, transform=ax2.transAxes, fontsize=4, verticalalignment='top')\n",
        "\n",
        "ax2.legend(lines, labels, bbox_to_anchor=(0.8, 1), loc='upper left', prop={'size': 4})\n",
        "\n",
        "fig2.savefig(\"scatter\", dpi=1000)\n",
        "from google.colab import files\n",
        "files.download(\"scatter.png\")\n",
        "plt.close(fig2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For CH2-stop epochs with SVDD below (3.9419)\n",
            "AP at min SVDD = 0.15\n",
            "average AP = 0.17\n",
            "ROCAUC at min SVDD = 0.63\n",
            "average ROCAUC = 0.70\n",
            "Knee point: (0.1365, -24417743.88)\n",
            "AP at knee = 0.15; ROCAUC at knee = 0.63\n",
            "\n",
            "\n",
            "For conv-stop epochs with SVDD below (5.9469)\n",
            "AP at min SVDD = 0.17\n",
            "average AP = 0.17\n",
            "ROCAUC at min SVDD = 0.74\n",
            "average ROCAUC = 0.69\n",
            "\n",
            "\n",
            "For 150th epochs with SVDD below (1.2974)\n",
            "AP at min SVDD = 0.17\n",
            "average AP = 0.17\n",
            "ROCAUC at min SVDD = 0.74\n",
            "average ROCAUC = 0.70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d9cddb1e-8662-45eb-b7f9-233b5b71f9b4\", \"scatter.png\", 589720)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5xi9avcBoxr"
      },
      "source": [
        "rm *.pkl"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}