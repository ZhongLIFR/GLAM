{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphDeepSVDD.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1QF3FrgStWvmS32Oe2lLkuq08ouBSkDIz",
      "authorship_tag": "ABX9TyMgBol61y3BuGRmDydpSJz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sawlani/GNN-anomaly/blob/master/GraphDeepSVDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmnuB6KRf6DV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import average_precision_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MSrb-w1qus"
      },
      "source": [
        "class S2VGraph(object):\n",
        "    def __init__(self, g, label, node_tags=None, node_features=None):\n",
        "        '''\n",
        "            g: a networkx graph\n",
        "            label: an integer graph label\n",
        "            node_tags: a list of integer node tags\n",
        "            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n",
        "            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n",
        "            neighbors: list of neighbors (without self-loop)\n",
        "        '''\n",
        "        self.label = label\n",
        "        self.g = g\n",
        "        self.node_tags = node_tags\n",
        "        self.node_features = node_features # one-hot encoded node-tags\n",
        "        self.edge_mat = None\n",
        "\n",
        "        # edge_mat is of the form:\n",
        "        # [[u1 u2 u3 ... um]\n",
        "        #  [v1 v2 v3 ... vm]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF-cMiOn2I4-"
      },
      "source": [
        "def random_split_counts(total_number, no_of_splits):\n",
        "    split_indices = np.sort(np.random.choice(total_number,no_of_splits-1, replace = False)+1)\n",
        "    split_indices = np.insert(split_indices, 0, 0)\n",
        "    split_indices = np.append(split_indices, total_number)\n",
        "    split_counts = np.diff(split_indices)\n",
        "    return split_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4sErmP72NIc"
      },
      "source": [
        "random_state = np.random.RandomState() #type: np.random.RandomState"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktymxOVK2kjD"
      },
      "source": [
        "class GraphGenerator:\n",
        "\tdef __init__(self, numClass):\n",
        "\t\tself.numClass = numClass\n",
        "\n",
        "\tdef format_path(self, G, savePath, graphName, **kwargs):\n",
        "\t\tgraphName = graphName.format(numNode=G.number_of_nodes(), numEdge=G.number_of_edges(), numClass=self.numClass, **kwargs)\n",
        "\t\tsavePath = savePath.format(graphName=graphName, numNode=G.number_of_nodes(), numEdge=G.number_of_edges(), numClass=self.numClass, **kwargs)\n",
        "\t\treturn savePath, graphName\n",
        "\n",
        "\tdef save_graph(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tG_d = nx.to_dict_of_lists(G)\n",
        "\t\tprint(\"Saving graph to {}\".format(os.path.join(savePath, graphName + \".graph\")))\n",
        "\t\tpickle.dump(G_d, open(os.path.join(savePath, graphName + \".graph\"), \"wb\"))\n",
        "\t\n",
        "\tdef save_y(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tally = np.zeros((len(G.nodes()), self.numClass))\n",
        "\t\tfor v in G.nodes():\n",
        "\t\t\tally[v][G.nodes[v]['color'] - 1] = 1\n",
        "\t\tprint(\"Saving labels to {}\".format(os.path.join(savePath, graphName + \".ally\")))\n",
        "\t\tpickle.dump(ally, open(os.path.join(savePath, graphName + \".ally\"), \"wb\"))\n",
        "\t\n",
        "\tdef save_nx_graph(self, G: nx.Graph, savePath, graphName, **kwargs):\n",
        "\t\tsavePath, graphName = self.format_path(G, savePath, graphName, **kwargs)\n",
        "\t\tprint(\"Pickling networkx graph to {}\".format(os.path.join(savePath, graphName + \".gpickle.gz\")))\n",
        "\t\tnx.write_gpickle(G, os.path.join(savePath, graphName + \".gpickle.gz\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ENcH4OA2lfL"
      },
      "source": [
        "class MixhopGraphGenerator(GraphGenerator):\n",
        "\tdef get_color(self, class_ratio): # Assign new node to a class\n",
        "\t\tif self.__coloriter:\n",
        "\t\t\treturn next(self.__coloriter)\n",
        "\t\telse:\n",
        "\t\t\treturn np.random.choice(list(range(1, len(class_ratio) + 1)), 1, False, class_ratio)[0]\n",
        "\n",
        "\tdef color_weight(self, col1, col2):\n",
        "\t\tdist = abs(col1 - col2)\n",
        "\t\tdist = min(dist, len(self.classRatio) - dist)\n",
        "\t\treturn self.heteroWeightsDict[dist]\n",
        "\t\n",
        "\tdef get_neighbors(self, G, m, col, h):\n",
        "\t\tpr = dict()\n",
        "\t\tfor v in G.nodes():\n",
        "\t\t\tdegree_v = float(max(G.degree[v], 1)) # Degree is treated as at least 1, so that pr[:] is not all 0\n",
        "\t\t\tif G.nodes[v]['color'] == col:\n",
        "\t\t\t\tpr[v] = float(degree_v) * h\n",
        "\t\t\telse:\n",
        "\t\t\t\tpr[v] = float(degree_v) * ((1 - h) * self.color_weight(col, G.nodes[v]['color']))\n",
        "\n",
        "\t\tnorm_pr = float(sum(pr.values()))\n",
        "\t\tif norm_pr == 0:\n",
        "\t\t\treturn None\n",
        "\t\telse:\n",
        "\t\t\tfor v in list(pr.keys()):\n",
        "\t\t\t\tpr[v] = float(pr[v]) / norm_pr\n",
        "\t\t\tneighbors = np.random.choice(list(pr.keys()), m, False, list(pr.values()))\n",
        "\t\t\treturn neighbors\n",
        "\n",
        "\tdef __init__(self, classRatio, heteroClsWeight=\"circularDist\", **kwargs):\n",
        "\t\tsuper().__init__(len(classRatio))\n",
        "\t\tself.classRatio = classRatio\n",
        "\t\tself.heteroWeightsDict = dict()\n",
        "\t\t\n",
        "\t\tif heteroClsWeight == \"circularDist\":\n",
        "\t\t\tfor i in range(2, self.numClass + 1):\n",
        "\t\t\t\tcircularDist = min(i - 1, self.numClass - (i - 1))\n",
        "\t\t\t\tself.heteroWeightsDict[circularDist] = self.heteroWeightsDict.get(circularDist, 0) + 1\n",
        "\n",
        "\t\t\tmaxDist = max(self.heteroWeightsDict.keys())\n",
        "\t\t\tweightSum = 0\n",
        "\t\t\tfor dist, times in self.heteroWeightsDict.items():\n",
        "\t\t\t\tself.heteroWeightsDict[dist] = kwargs[\"heteroWeightsExponent\"] ** (maxDist - dist)\n",
        "\t\t\t\tweightSum += self.heteroWeightsDict[dist] * times\n",
        "\t\t\tself.heteroWeightsDict = {dist: weight / weightSum for dist, weight in self.heteroWeightsDict.items()}\n",
        "\t\t\n",
        "\t\telif heteroClsWeight == \"uniform\":\n",
        "\t\t\tfor i in range(2, self.numClass + 1):\n",
        "\t\t\t\tcircularDist = min(i - 1, self.numClass - (i - 1))\n",
        "\t\t\t\tself.heteroWeightsDict[circularDist] = 1.0 / len(range(2, self.numClass + 1))\n",
        "\n",
        "\tdef generate_graph(self, n, m, m0, h):\n",
        "\t\t'''\n",
        "\t\tn: Target size for the generated network\n",
        "\t\tm: number of edges added with each new node\n",
        "\t\tm0: number of nodes to begin with\n",
        "\t\th: homophily\n",
        "\t\t'''\n",
        "\t\tif n > 1 and np.sum(self.classRatio) == n:\n",
        "\t\t\t#print(\"Graph will be generated with size of each class exactly equal to the number specified in classRatio.\")\n",
        "\t\t\tself.__colorlist = []\n",
        "\t\t\tfor classID, classSize in enumerate(self.classRatio):\n",
        "\t\t\t\tself.__colorlist += [classID + 1] * int(classSize - m)\n",
        "\t\t\trandom_state.shuffle(self.__colorlist)\n",
        "\t\t\thead_list = list(range(1, self.numClass + 1)) * m\n",
        "\t\t\trandom_state.shuffle(head_list)\n",
        "\t\t\tself.__colorlist = head_list + self.__colorlist\n",
        "\t\t\tself.__coloriter = iter(self.__colorlist)\n",
        "\t\telse:\n",
        "\t\t\tself.__coloriter = None\n",
        "\t\t\n",
        "\t\tif m * self.numClass > m0:\n",
        "\t\t\traise ValueError(\"Barabasi-Albert model requires m to be less or equal to m0\")\n",
        "\n",
        "\t\tif m > n:\n",
        "\t\t\traise ValueError(\"m > n should be satisfied\")\n",
        "\n",
        "\t\tG = nx.Graph()\n",
        "\n",
        "\t\tfor v in range(m0):\n",
        "\t\t\tnext_color = self.get_color(self.classRatio)\n",
        "\t\t\tif v > 1:\n",
        "\t\t\t\tif h != 0 and h != 1:\n",
        "\t\t\t\t\tnext_neighbor = v - 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnext_n = self.get_neighbors(G, 1, next_color, h)\n",
        "\t\t\t\t\tif next_n is not None:\n",
        "\t\t\t\t\t\tnext_neighbor = next_n[0]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnext_neighbor = None\n",
        "\n",
        "\t\t\tG.add_node(v, color=next_color)\n",
        "\t\t\tif v > 1 and next_neighbor is not None:\n",
        "\t\t\t\tG.add_edge(v, next_neighbor)\n",
        "\t\t\t\n",
        "\t\tfor v in range(m0, n):\n",
        "\t\t\tif v % 1000 == 0:\n",
        "\t\t\t\tprint(\"Generating graph... Now processing v = {}\".format(v))\n",
        "\t\t\tcol = self.get_color(self.classRatio)\n",
        "\t\t\tus = self.get_neighbors(G, m, col, h)\n",
        "\n",
        "\t\t\tG.add_node(v, color=col)\n",
        "\t\t\tassert us is not None\n",
        "\t\t\tfor u in us:\n",
        "\t\t\t\tG.add_edge(v, u)\n",
        "\n",
        "\t\tassert len(list(nx.selfloop_edges(G))) == 0\n",
        "\t\treturn G\n",
        "\n",
        "\tdef generate_graph_contaminated(self, n, m, m0, h, contamination = 0.2):\n",
        "\t\t'''\n",
        "\t\tn: Target size for the generated network\n",
        "\t\tm: number of edges added with each new node\n",
        "\t\tm0: number of nodes to begin with\n",
        "\t\th: homophily\n",
        "\t\t'''\n",
        "\t\tif n > 1 and np.sum(self.classRatio) == n:\n",
        "\t\t\t#print(\"Graph will be generated with size of each class exactly equal to the number specified in classRatio.\")\n",
        "\t\t\tself.__colorlist = []\n",
        "\t\t\tfor classID, classSize in enumerate(self.classRatio):\n",
        "\t\t\t\tself.__colorlist += [classID + 1] * int(classSize - m)\n",
        "\t\t\trandom_state.shuffle(self.__colorlist)\n",
        "\t\t\thead_list = list(range(1, self.numClass + 1)) * m\n",
        "\t\t\trandom_state.shuffle(head_list)\n",
        "\t\t\tself.__colorlist = head_list + self.__colorlist\n",
        "\t\t\tself.__coloriter = iter(self.__colorlist)\n",
        "\t\telse:\n",
        "\t\t\tself.__coloriter = None\n",
        "\t\t\n",
        "\t\tif m * self.numClass > m0:\n",
        "\t\t\traise ValueError(\"Barabasi-Albert model requires m to be less or equal to m0\")\n",
        "\n",
        "\t\tif m > n:\n",
        "\t\t\traise ValueError(\"m > n should be satisfied\")\n",
        "\n",
        "\t\tG = nx.Graph()\n",
        "\n",
        "\t\tfor v in range(m0):\n",
        "\t\t\tnext_color = self.get_color(self.classRatio)\n",
        "\t\t\tif v > 1:\n",
        "\t\t\t\tif h != 0 and h != 1:\n",
        "\t\t\t\t\tnext_neighbor = v - 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnext_n = self.get_neighbors(G, 1, next_color, h)\n",
        "\t\t\t\t\tif next_n is not None:\n",
        "\t\t\t\t\t\tnext_neighbor = next_n[0]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnext_neighbor = None\n",
        "\n",
        "\t\t\tG.add_node(v, color=next_color)\n",
        "\t\t\tif v > 1 and next_neighbor is not None:\n",
        "\t\t\t\tG.add_edge(v, next_neighbor)\n",
        "\t\t\t\n",
        "\t\tfor v in range(m0, n):\n",
        "\t\t\tif v % 1000 == 0:\n",
        "\t\t\t\tprint(\"Generating graph... Now processing v = {}\".format(v))\n",
        "\t\t\tcol = self.get_color(self.classRatio)\n",
        "\n",
        "\t\t\tr = np.random.uniform()\n",
        "\t\t\tif r < contamination/2:\n",
        "\t\t\t\tchanged_h = h + 0.25\n",
        "\t\t\telif r < contamination:\n",
        "\t\t\t\tchanged_h = h - 0.25\n",
        "\t\t\telse:\n",
        "\t\t\t\tchanged_h = h\n",
        "\t\t\tus = self.get_neighbors(G, m, col, changed_h)\n",
        "\n",
        "\t\t\tG.add_node(v, color=col)\n",
        "\t\t\tassert us is not None\n",
        "\t\t\tfor u in us:\n",
        "\t\t\t\tG.add_edge(v, u)\n",
        "\n",
        "\t\tassert len(list(nx.selfloop_edges(G))) == 0\n",
        "\t\treturn G\n",
        "\n",
        "\tdef __call__(self, n, m, m0, h):\n",
        "\t\treturn self.generate_graph(n, m, m0, h)\n",
        "\n",
        "\tdef save_graph(self, G:nx.Graph, savePath=\"{graphName}\", graphName=\"{method}-n{numNode}-h{h}-c{numClass}\", **kwargs):\n",
        "\t\tsuper().save_graph(G, savePath, graphName, method=\"mixhop\", **kwargs)\n",
        "\n",
        "\tdef save_y(self, G:nx.Graph, savePath=\"{graphName}\", graphName=\"{method}-n{numNode}-h{h}-c{numClass}\", **kwargs):\n",
        "\t\tsuper().save_y(G, savePath, graphName, method=\"mixhop\", **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-qhjCfI20Ms"
      },
      "source": [
        "def load_synthetic_data(number_of_graphs = 100, h_inlier=0, h_outlier=1, outlier_ratio=0.5, n_min = 50, n_max = 150, no_of_tags = 5, type1 = \"mixhop\", type2 = \"mixhop\"):\n",
        "    print('generating data')\n",
        "    g_list = []\n",
        "    \n",
        "    number_of_outliers = int(number_of_graphs*outlier_ratio)\n",
        "\n",
        "    for i in range(number_of_graphs - number_of_outliers):\n",
        "        \n",
        "        n = np.random.randint(n_min, n_max)\n",
        "        tag_counts = random_split_counts(n, no_of_tags)\n",
        "\n",
        "        if type1 == \"mixhop\":\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0)(n, 2, 10, h_inlier)\n",
        "            tags = [g.nodes[v]['color'] for v in g.nodes]\n",
        "        \n",
        "        g_list.append(S2VGraph(g, 0, node_tags=tags))\n",
        "    #draw_graph(g, \"g1.jpg\")\n",
        "    \n",
        "    for i in range(number_of_graphs - number_of_outliers, number_of_graphs):\n",
        "        \n",
        "        n = np.random.randint(n_min, n_max)\n",
        "        tag_counts = random_split_counts(n, no_of_tags)\n",
        "\n",
        "        if type2 == \"mixhop\":\n",
        "            g = MixhopGraphGenerator(tag_counts, heteroWeightsExponent=1.0)(n, 2, 10, h_outlier)\n",
        "            tags = [g.nodes[v]['color'] for v in g.nodes]\n",
        "        \n",
        "        g_list.append(S2VGraph(g, 1, node_tags=tags))\n",
        "    #draw_graph(g, \"g2.jpg\")\n",
        "    \n",
        "    for g in g_list:\n",
        "        edges = [list(pair) for pair in g.g.edges()]\n",
        "        edges.extend([[i, j] for j, i in edges])\n",
        "\n",
        "        g.edge_mat = torch.LongTensor(edges).transpose(0,1)\n",
        "\n",
        "    if g.node_tags == None:\n",
        "        print(\"no node tags provided, using degrees as tags\")\n",
        "        for g in g_list:\n",
        "            g.node_tags = list(dict(g.g.degree).values())\n",
        "\n",
        "\n",
        "    # Extracting unique tags and converting to one-hot features   \n",
        "    tagset = set()\n",
        "    for g in g_list:\n",
        "        tagset = tagset.union(set(g.node_tags))\n",
        "\n",
        "    tagset = list(tagset)\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\n",
        "\n",
        "    for g in g_list:\n",
        "        g.node_features = torch.zeros(len(g.node_tags), len(tagset))\n",
        "        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1\n",
        "\n",
        "    print('Maximum node tag: %d' % len(tagset))\n",
        "    print(\"Number of graphs generated: %d\" % (number_of_graphs))\n",
        "    print(\"Number of outlier graphs generated: %d\" % (number_of_outliers))\n",
        "\n",
        "    return g_list, 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sh_4Tzqhd2f"
      },
      "source": [
        "def compute_gamma(embeddings, device=torch.device(\"cpu\")):\n",
        "    all_vertex_embeddings = torch.cat(embeddings, axis=0).detach().to(device)\n",
        "    all_vertex_distances = torch.cdist(all_vertex_embeddings, all_vertex_embeddings)**2\n",
        "    median_of_distances = torch.median(all_vertex_distances)\n",
        "    if median_of_distances <= 1e-4:\n",
        "        median_of_distances = 1e-4\n",
        "    \n",
        "    gamma = 1/median_of_distances\n",
        "\n",
        "    return gamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd5GamXChiTD"
      },
      "source": [
        "def compute_mmd_gram_matrix(X_embeddings, Y_embeddings=None, gamma=None, type=\"SMM\", device=torch.device(\"cpu\")):\n",
        "    \n",
        "    if not Y_embeddings:\n",
        "        Y_embeddings = X_embeddings\n",
        "\n",
        "    if gamma == None:\n",
        "        gamma = compute_gamma(Y_embeddings)\n",
        "    if gamma==0:\n",
        "        raise ValueError(\"Gamma value appears to be 0\")\n",
        "    \n",
        "    # pad with 0s and convert to 3d tensor. \n",
        "    X_padded = pad_sequence(X_embeddings, batch_first=True).to(device)\n",
        "    Y_padded = pad_sequence(Y_embeddings, batch_first=True).to(device)\n",
        "\n",
        "    # calculate mask to be able to exclude padded 0s later while computing mean\n",
        "    X_ones = [torch.ones(emb.shape[0]) for emb in X_embeddings]\n",
        "    Y_ones = [torch.ones(emb.shape[0]) for emb in Y_embeddings]\n",
        "    X_ones_padded = pad_sequence(X_ones, batch_first=True).to(device)\n",
        "    Y_ones_padded = pad_sequence(Y_ones, batch_first=True).to(device)\n",
        "    mask = X_ones_padded[:,None,:,None]*Y_ones_padded[None,:,None,:]\n",
        "\n",
        "    XY = torch.matmul(X_padded[:,None,:,:], torch.transpose(Y_padded[None,:,:,:], -1, -2))\n",
        "\n",
        "    if type==\"SMM\":\n",
        "        X_sq = torch.squeeze(torch.matmul(X_padded[:,:,None,:], X_padded[:,:,:,None]))\n",
        "        Y_sq = torch.squeeze(torch.matmul(Y_padded[:,:,None,:], Y_padded[:,:,:,None]))\n",
        "        \n",
        "        K_XY = torch.exp(-gamma * (-2 * XY + X_sq[:,None,:,None] + Y_sq[None,:,None,:]))\n",
        "\n",
        "        masked_means = torch.true_divide(torch.sum(K_XY*mask,(2,3)), torch.sum(mask,(2,3)))\n",
        "    else:\n",
        "        raise ValueError(\"This type is not supported (yet)\")\n",
        "    \n",
        "    return masked_means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xOcXANXhwni"
      },
      "source": [
        "class MLP2layer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, bias=False):\n",
        "        '''\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            output_dim: number of classes for prediction\n",
        "        '''\n",
        "    \n",
        "        super(MLP2layer, self).__init__()\n",
        "\n",
        "        self.linear_in = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
        "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
        "        self.linear_out = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear_in(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear_out(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUQkJFlwhy9U"
      },
      "source": [
        "class GraphCNN_SVDD(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, learn_eps, neighbor_pooling_type, bias, device):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks (INCLUDING the input layer)\n",
        "            input_dim: dimensionality of input features\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            output_dim: number of classes for prediction\n",
        "            learn_eps: If True, learn epsilon to distinguish center nodes from neighboring nodes. If False, aggregate neighbors and center nodes altogether. \n",
        "            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)\n",
        "            device: which device to use\n",
        "        '''\n",
        "\n",
        "        super(GraphCNN_SVDD, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.neighbor_pooling_type = neighbor_pooling_type\n",
        "        self.learn_eps = learn_eps\n",
        "        self.eps = nn.Parameter(torch.zeros(self.num_layers-1))\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.mlps = torch.nn.ModuleList()\n",
        "\n",
        "        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers-1):\n",
        "            if layer == 0:\n",
        "                self.mlps.append(MLP2layer(input_dim, hidden_dim, hidden_dim, bias=bias))\n",
        "            else:\n",
        "                self.mlps.append(MLP2layer(hidden_dim, hidden_dim, hidden_dim, bias=bias))\n",
        "\n",
        "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        #Linear function that maps the hidden representation at dofferemt layers into a prediction score\n",
        "        #self.linears_prediction = torch.nn.ModuleList()\n",
        "        #for layer in range(num_layers):\n",
        "        #    if layer == 0:\n",
        "        #        self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n",
        "        #    else:\n",
        "        #        self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        \n",
        "\n",
        "    def __preprocess_neighbors_maxpool(self, batch_graph):\n",
        "        ###create padded_neighbor_list in concatenated graph\n",
        "\n",
        "        #compute the maximum number of neighbors within the graphs in the current minibatch\n",
        "        max_deg = max([graph.max_neighbor for graph in batch_graph])\n",
        "\n",
        "        padded_neighbor_list = []\n",
        "        start_idx = [0]\n",
        "\n",
        "\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            start_idx.append(start_idx[i] + len(graph.g))\n",
        "            padded_neighbors = []\n",
        "            for j in range(len(graph.neighbors)):\n",
        "                #add off-set values to the neighbor indices\n",
        "                pad = [n + start_idx[i] for n in graph.neighbors[j]]\n",
        "                #padding, dummy data is assumed to be stored in -1\n",
        "                pad.extend([-1]*(max_deg - len(pad)))\n",
        "\n",
        "                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
        "                if not self.learn_eps:\n",
        "                    pad.append(j + start_idx[i])\n",
        "\n",
        "                padded_neighbors.append(pad)\n",
        "            padded_neighbor_list.extend(padded_neighbors)\n",
        "\n",
        "        return torch.LongTensor(padded_neighbor_list)\n",
        "\n",
        "\n",
        "    def __preprocess_neighbors_sumavepool(self, batch_graph):\n",
        "        ###create block diagonal sparse matrix\n",
        "\n",
        "        edge_mat_list = []\n",
        "        start_idx = [0]\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            start_idx.append(start_idx[i] + len(graph.g))\n",
        "            edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
        "        Adj_block_idx = torch.cat(edge_mat_list, 1)\n",
        "        Adj_block_elem = torch.ones(Adj_block_idx.shape[1])\n",
        "\n",
        "        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
        "\n",
        "        if not self.learn_eps:\n",
        "            num_node = start_idx[-1]\n",
        "            self_loop_edge = torch.LongTensor([range(num_node), range(num_node)])\n",
        "            elem = torch.ones(num_node)\n",
        "            Adj_block_idx = torch.cat([Adj_block_idx, self_loop_edge], 1)\n",
        "            Adj_block_elem = torch.cat([Adj_block_elem, elem], 0)\n",
        "\n",
        "        Adj_block = torch.sparse.FloatTensor(Adj_block_idx, Adj_block_elem, torch.Size([start_idx[-1],start_idx[-1]]))\n",
        "\n",
        "        return Adj_block.to(self.device)\n",
        "\n",
        "\n",
        "    def maxpool(self, h, padded_neighbor_list):\n",
        "        ###Element-wise minimum will never affect max-pooling\n",
        "\n",
        "        dummy = torch.min(h, dim = 0)[0]\n",
        "        h_with_dummy = torch.cat([h, dummy.reshape((1, -1)).to(self.device)])\n",
        "        pooled_rep = torch.max(h_with_dummy[padded_neighbor_list], dim = 1)[0]\n",
        "        return pooled_rep\n",
        "\n",
        "\n",
        "    def next_layer_eps(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes separately by epsilon reweighting. \n",
        "\n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            ##If max pooling\n",
        "            pooled = self.maxpool(h, padded_neighbor_list)\n",
        "        else:\n",
        "            #If sum or average pooling\n",
        "            pooled = torch.spmm(Adj_block, h)\n",
        "            if self.neighbor_pooling_type == \"average\":\n",
        "                #If average pooling\n",
        "                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n",
        "                pooled = pooled/degree\n",
        "\n",
        "        #Reweights the center node representation when aggregating it with its neighbors\n",
        "        pooled = pooled + (1 + self.eps[layer])*h\n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "        h = self.batch_norms[layer](pooled_rep)\n",
        "\n",
        "        #non-linearity\n",
        "        h = F.relu(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes altogether  \n",
        "            \n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            ##If max pooling\n",
        "            pooled = self.maxpool(h, padded_neighbor_list)\n",
        "        else:\n",
        "            #If sum or average pooling\n",
        "            pooled = torch.spmm(Adj_block, h)\n",
        "            if self.neighbor_pooling_type == \"average\":\n",
        "                #If average pooling\n",
        "                degree = torch.spmm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n",
        "                pooled = pooled/degree\n",
        "\n",
        "        #representation of neighboring and center nodes \n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "\n",
        "        h = self.batch_norms[layer](pooled_rep)\n",
        "\n",
        "        #non-linearity\n",
        "        h = F.relu(h)\n",
        "        return h\n",
        "\n",
        "    def forward(self, batch_graph, output_layer):\n",
        "        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)\n",
        "        \n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)\n",
        "        else:\n",
        "            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)\n",
        "\n",
        "        #list of hidden representation at each layer (including input)\n",
        "        hidden_rep = [X_concat]\n",
        "        h = X_concat\n",
        "\n",
        "        for layer in range(self.num_layers-1):\n",
        "            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
        "                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
        "            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
        "                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)\n",
        "            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
        "                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
        "            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
        "                h = self.next_layer(h, layer, Adj_block = Adj_block)\n",
        "\n",
        "            hidden_rep.append(h)\n",
        "\n",
        "        if output_layer==\"all\":\n",
        "            hidden_rep = torch.cat(hidden_rep, axis=1)\n",
        "        else:\n",
        "            hidden_rep = hidden_rep[output_layer]\n",
        "    \n",
        "        index = 0\n",
        "        embeddings = []\n",
        "        for graph in batch_graph:\n",
        "            embedding = hidden_rep[index:index+len(graph.g)]\n",
        "            index += len(graph.g)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "\n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGNZ5og46V7D"
      },
      "source": [
        "def train_fullbatch(args, model, train_graphs, optimizer, epoch, Z, center=None, linear_layer=None, linear_layer_optimizer=None):\n",
        "    model.eval() # we don't need batch-norm to track mean/variance\n",
        "    \n",
        "    loss_accum = 0\n",
        "    svdd_loss_accum = 0\n",
        "    reg_loss_accum = 0\n",
        "    total_iters = args.iters_per_epoch\n",
        "    #pbar = tqdm(range(total_iters), unit='iter')\n",
        "    #pbar.set_description('epoch: %d' % (epoch))\n",
        "\n",
        "    #for pos in pbar:\n",
        "    for _ in range(total_iters):\n",
        "\n",
        "        Z_embeddings = model(Z, args.layer)\n",
        "        gamma = compute_gamma(Z_embeddings, device=args.device).detach() # no backpropagation for gamma\n",
        "        \n",
        "        K_Z = compute_mmd_gram_matrix(Z_embeddings, gamma=gamma, device=args.device).to(args.device)\n",
        "        eigenvalues, U_Z = torch.symeig(K_Z, eigenvectors=True)\n",
        "        T = torch.matmul(U_Z,torch.diag(eigenvalues**-0.5))\n",
        "\n",
        "        R_embeddings = model(train_graphs,args.layer)\n",
        "        K_RZ = compute_mmd_gram_matrix(R_embeddings, Z_embeddings, gamma=gamma, device=args.device).to(args.device)\n",
        "        F = torch.matmul(K_RZ, T)\n",
        "\n",
        "        if center == None:\n",
        "            center = torch.median(F, dim=0).values.detach() # no backpropagation for center\n",
        "        \n",
        "        dists = torch.sum((F - center)**2, dim=1).cpu()\n",
        "        \n",
        "        #dists = torch.sum(\n",
        "        #        (torch.abs(F-center) <= args.delta)*((F-center)**2) +\n",
        "        #                      (torch.abs(F-center) > args.delta)*2*(args.delta*torch.abs((F-center))-(args.delta**2)),\n",
        "        #                      dim=1).cpu()\n",
        "        \n",
        "        ## Update hypersphere radius R on mini-batch distances\n",
        "        #if epoch > args.warm_up_n_epochs:\n",
        "        #    if args.train_only_inlier:\n",
        "        #        args.radius = np.sqrt(np.max(dists.clone().data.cpu().numpy()))\n",
        "        #    else:\n",
        "        #        args.radius = np.sqrt(np.quantile(dists.clone().data.cpu().numpy(), 1 - args.nu))\n",
        "        \n",
        "        scores = torch.clamp(dists - (args.radius**2), min=0)\n",
        "        svdd_loss = (1/args.nu)*torch.mean(scores)\n",
        "        \n",
        "        if args.regularizer == \"variance\":\n",
        "            reg_loss = (1/(F.shape[0]-1))*torch.sum(torch.var(F,dim=0))\n",
        "            \n",
        "        elif args.regularizer == \"classification\":\n",
        "            random_labels = torch.empty(len(train_graphs),args.no_of_random_labels).random_(2).to(args.device)\n",
        "            raw_scores = linear_layer(F).to(args.device)\n",
        "\n",
        "            criterion = nn.BCELoss()\n",
        "            m = nn.Sigmoid()\n",
        "\n",
        "            reg_loss = criterion(m(raw_scores), random_labels).cpu()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unrecognized regularization type\")\n",
        "\n",
        "        loss = svdd_loss - args.reg_weight * reg_loss\n",
        "        args.reg_weight = (args.alpha*args.reg_weight + (1-args.alpha)*args.beta*(svdd_loss/reg_loss)).detach()\n",
        "            \n",
        "        #backpropagate\n",
        "        optimizer.zero_grad()\n",
        "        if args.regularizer == \"classification\":\n",
        "            linear_layer_optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()    \n",
        "\n",
        "        optimizer.step()\n",
        "        if args.regularizer == \"classification\":\n",
        "            linear_layer_optimizer.step()\n",
        "\n",
        "        loss_accum += loss.detach().cpu().numpy()\n",
        "        svdd_loss_accum += svdd_loss.detach().cpu().numpy()\n",
        "        reg_loss_accum += reg_loss.detach().cpu().numpy()\n",
        "\n",
        "    average_loss = loss_accum/total_iters\n",
        "    average_svdd_loss = svdd_loss_accum/total_iters\n",
        "    average_reg_loss = reg_loss_accum/total_iters\n",
        "\n",
        "    return average_loss, average_svdd_loss, average_reg_loss, center"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CorsXoCTiADs"
      },
      "source": [
        "def test(args, model, test_graphs, Z, center=None):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        Z_embeddings = model(Z, args.layer)\n",
        "        gamma = compute_gamma(Z_embeddings, args.device)\n",
        "        \n",
        "        K_Z = compute_mmd_gram_matrix(Z_embeddings, gamma=gamma, device=args.device).to(args.device)\n",
        "        eigenvalues, U_Z = torch.symeig(K_Z, eigenvectors=True)\n",
        "        T = torch.matmul(U_Z,torch.diag(eigenvalues**-0.5))\n",
        "\n",
        "        R_embeddings = model(test_graphs,args.layer)\n",
        "        K_RZ = compute_mmd_gram_matrix(R_embeddings, Z_embeddings, gamma=gamma, device=args.device).to(args.device)\n",
        "        F = torch.matmul(K_RZ, T)\n",
        "        \n",
        "        if center == None:\n",
        "            center = torch.median(F, dim=0).values\n",
        "        dists = torch.sum((F - center)**2, dim=1).cpu()\n",
        "\n",
        "        labels = torch.LongTensor([graph.label for graph in test_graphs])\n",
        "        \n",
        "        score = average_precision_score(labels, dists)\n",
        "        return score, dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf8wBuL7iZAz"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='PyTorch graph convolutional neural net for whole-graph classification')\n",
        "parser.add_argument('--device', type=int, default=0,\n",
        "                    help='which gpu to use if any (default: 0)')\n",
        "parser.add_argument('--batch_size', type=int, default=100,\n",
        "                    help='input batch size for training (default: 100)')\n",
        "parser.add_argument('--iters_per_epoch', type=int, default=1,\n",
        "                    help='number of iterations per each epoch (default: 1)')\n",
        "parser.add_argument('--epochs', type=int, default=500,\n",
        "                    help='number of epochs to train (default: 500)')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='learning rate (default: 0.01)')\n",
        "parser.add_argument('--weight_decay', type=float, default=0,\n",
        "                    help='weight_decay constant (lambda), default=0.')\n",
        "\n",
        "\n",
        "parser.add_argument('--num_layers', type=int, default=5,\n",
        "                    help='number of layers INCLUDING the input one (default: 5)')\n",
        "parser.add_argument('--hidden_dim', type=int, default=64,\n",
        "                    help='number of hidden units (default: 64)')\n",
        "parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\", \"max\"],\n",
        "                    help='Pooling for over neighboring nodes: sum, average or max')\n",
        "parser.add_argument('--dont_learn_eps', action=\"store_true\",\n",
        "                                    help='Whether to learn the epsilon weighting for the center nodes. Does not affect training accuracy though.')\n",
        "parser.add_argument('--bias', action=\"store_true\",\n",
        "                                    help='Whether to use bias terms in the GNN.')\n",
        "parser.add_argument('--degree_as_tag', action=\"store_true\",\n",
        "                    help='let the input node features be the degree of nodes (heuristics for unlabeled graph)')\n",
        "parser.add_argument('--layer', type = str, default = \"all\",\n",
        "                                    help='which hidden layer used as embedding')\n",
        "\n",
        "\n",
        "parser.add_argument('--dataset', type = str, default = \"mixhop\", choices=[\"mixhop\", \"chem\", \"contaminated\", \"saved\"],\n",
        "                                    help='dataset used')\n",
        "parser.add_argument('--no_of_graphs', type = int, default = 100,\n",
        "                                    help='no of graphs generated')\n",
        "parser.add_argument('--h_inlier', type=float, default=0.3,\n",
        "                    help='inlier homophily (default: 0.3)')\n",
        "parser.add_argument('--h_outlier', type=float, default=0.7,\n",
        "                    help='outlier homophily (default: 0.7)')\n",
        "parser.add_argument('--nu', type=float, default=0.05,\n",
        "                    help='expected fraction of outliers (default: 0.05)')\n",
        "parser.add_argument('--k_frac', type=float, default=0.4,\n",
        "                    help='fraction of landmark points (default: 0.4)')\n",
        "\n",
        "\n",
        "parser.add_argument('--radius', type=str, default=\"0\",\n",
        "                    help='hypersphere radius (default: 0)')\n",
        "parser.add_argument('--train_only_inlier', action=\"store_true\",\n",
        "                                    help='Train only using inlier data')\n",
        "#parser.add_argument('--delta', type=float, default=1e9,\n",
        "#                                    help='threshold for huber loss')\n",
        "\n",
        "\n",
        "parser.add_argument('--regularizer', type=str, default=\"variance\", choices=[\"variance\", \"classification\"], \n",
        "                    help='type of regularizer (default: variance)')\n",
        "parser.add_argument('--reg_weight', type=float, default=0,\n",
        "                                    help='weight for variance regularizer')\n",
        "parser.add_argument('--alpha', type=float, default=0.9,\n",
        "                    help='regularizer: speed of adaptivity')\n",
        "parser.add_argument('--beta', type=float, default=0.5,\n",
        "                    help='regularizer loss multiplier (ratio)')\n",
        "#parser.add_argument('--warm_up_n_epochs', type=float, default=10,\n",
        "#                    help='epochs before radius is updated (default: 10)')\n",
        "\n",
        "parser.add_argument('--no_of_random_labels', type=int, default=10,\n",
        "                    help='number of random labels (default: 10')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78CzXMxRinWq"
      },
      "source": [
        "args = parser.parse_args(\"--train_only_inlier --layer=1 --dataset=mixhop --alpha=1.0 --beta=0.75\".split())\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "filename = \"beta_\"\n",
        "filename += str(args.beta)\n",
        "filename += \"_alpha_\"\n",
        "filename += str(args.alpha)\n",
        "#if args.reg_weight_adaptive:\n",
        "#    filename += \"adaptive\"\n",
        "#else:\n",
        "#    filename += str(args.reg_weight)\n",
        "if args.bias:\n",
        "    filename += \"_biasallowed\"\n",
        "if args.train_only_inlier:\n",
        "    filename += \"_inlieronly\"\n",
        "filename += \".png\"\n",
        "\n",
        "args.device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "if args.radius != \"dynamic\":\n",
        "    args.radius = float(args.radius)\n",
        "    args.warm_up_n_epochs = args.epochs # never update radius dynamically\n",
        "else:\n",
        "    args.radius = 0\n",
        "\n",
        "if args.layer != \"all\":\n",
        "    args.layer = int(args.layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq9pjCOkB5Y4"
      },
      "source": [
        "if args.dataset == \"mixhop\":\n",
        "    graphs, num_classes = load_synthetic_data(number_of_graphs=args.no_of_graphs, h_inlier=args.h_inlier, h_outlier=args.h_outlier, outlier_ratio=args.nu)\n",
        "    with open('graphs.pkl', 'wb') as f:\n",
        "        pickle.dump((graphs, num_classes), f)\n",
        "elif args.dataset == \"contaminated\":\n",
        "    graphs, num_classes = load_synthetic_data_contaminated(number_of_graphs=args.no_of_graphs, outlier_ratio=args.nu)\n",
        "elif args.dataset == \"chem\":\n",
        "    graphs, num_classes = load_chem_data()\n",
        "elif args.dataset == \"saved\":\n",
        "    with open('graphs.pkl', 'rb') as f:\n",
        "        graphs, num_classes = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2uUc4mP4_Zh"
      },
      "source": [
        "if args.train_only_inlier:\n",
        "    train_graphs, test_graphs = graphs[:int(args.no_of_graphs*(1-args.nu))], graphs\n",
        "else:\n",
        "    train_graphs, test_graphs = graphs, graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJZfs7bma2LS"
      },
      "source": [
        "k = int(args.k_frac*args.no_of_graphs)\n",
        "np.random.seed(0)\n",
        "Z = np.random.permutation(graphs[:int(args.no_of_graphs*(1-args.nu))])[:k] # pick landmark set only from inliers\n",
        "#print(Z)\n",
        "no_of_node_features = graphs[0].node_features.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n_xhRZOjEUd"
      },
      "source": [
        "model = GraphCNN_SVDD(args.num_layers, no_of_node_features, args.hidden_dim, num_classes, (not args.dont_learn_eps), args.neighbor_pooling_type, args.bias, args.device).to(args.device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.regularizer == \"classification\":\n",
        "    linear_layer = nn.Linear(k, args.no_of_random_labels).to(args.device)\n",
        "    linear_layer_optimizer = optim.SGD(linear_layer.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "else:\n",
        "    linear_layer = None\n",
        "    linear_layer_optimizer = None\n",
        "\n",
        "aps = []\n",
        "#outlier_ratios = []\n",
        "\n",
        "svdd_losses = []\n",
        "reg_losses = []\n",
        "total_losses = []\n",
        "\n",
        "#PRE-TRAINING TEST\n",
        "score, dists = test(args, model, test_graphs, Z)\n",
        "print(\"Pre-Training AP Score: %f\" % score)\n",
        "aps.append(score)\n",
        "dists = (dists**0.5).detach().numpy()\n",
        "\n",
        "\n",
        "#outlier_ratio = sum(dists > args.radius)/len(dists)\n",
        "#outlier_ratios.append(outlier_ratio)\n",
        "\n",
        "distlist= []\n",
        "distlist.append(dists)\n",
        "\n",
        "no_epochs = args.epochs\n",
        "train = train_fullbatch\n",
        "center = None\n",
        "\n",
        "for epoch in range(1, no_epochs + 1):\n",
        "\n",
        "    # training\n",
        "    loss, svdd_loss, reg_loss, center = train(args, model, train_graphs, optimizer, epoch, Z, center, linear_layer, linear_layer_optimizer)\n",
        "    \n",
        "\n",
        "    ## Calculate Weight Decay Loss\n",
        "    # model_reg_loss = 0\n",
        "    #for param in model.parameters():\n",
        "    #    model_reg_loss += 0.5 * args.weight_decay * torch.sum(param ** 2)\n",
        "    \n",
        "    score, dists = test(args, model, test_graphs, Z, center)\n",
        "    dists = (dists**0.5).detach().numpy()\n",
        "    \n",
        "    #outlier_ratio = sum(dists > args.radius)/len(dists)\n",
        "\n",
        "    distlist.append(dists)\n",
        "    \n",
        "    print(\"Epoch %d\" % epoch, end=\"\\t\")\n",
        "    print(\"SVDD loss: %f\" % (svdd_loss), end=\"\\t\")\n",
        "    print(\"Regularizer loss: %f\" % (reg_loss), end=\"\\t\")\n",
        "    print(\"Avg Precision Score: %f\" % score)\n",
        "    #print(\"Outlier ratio: %f\" % outlier_ratio)\n",
        "\n",
        "    aps.append(score)\n",
        "    #outlier_ratios.append(outlier_ratio)\n",
        "    \n",
        "    svdd_losses.append(svdd_loss)\n",
        "    reg_losses.append(reg_loss)\n",
        "    total_losses.append(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHioe20kCeHy"
      },
      "source": [
        "gap = 10\n",
        "\n",
        "intermittent_distlist = distlist[::gap]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "xs2 = []\n",
        "ys2 = []\n",
        "\n",
        "for i, dists in enumerate(intermittent_distlist):\n",
        "    for dist in dists[:95]:\n",
        "        xs.append(gap*i)\n",
        "        ys.append(dist)\n",
        "    for dist in dists[95:]:\n",
        "        xs2.append(gap*i)\n",
        "        ys2.append(dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUCWtY4G30TA"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "fig, axs = plt.subplots(2)\n",
        "fig.suptitle(\"SVDD with Nystrom for fixed radius=%f\" % args.radius)\n",
        "fig.tight_layout(pad=2, h_pad=2, w_pad=2)\n",
        "\n",
        "axs[0].set(ylabel='Average Precision', ylim=((0,1)))\n",
        "axs[0].plot(list(range(0, no_epochs + 1)), aps)\n",
        "axs[0].grid()\n",
        "\n",
        "axs[1].set(xlabel='Epochs', ylabel='Distances')\n",
        "axs[1].scatter(xs, ys, s=1, color='blue')\n",
        "axs[1].scatter(xs2, ys2, s=1, color='red')\n",
        "axs[1].grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(filename, dpi=1000)\n",
        "#files.download(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeOiZ_vD0Ne9"
      },
      "source": [
        "'''\n",
        "fig2, axs2 = plt.subplots(3)\n",
        "fig2.suptitle(\"Losses (regularizer = \" + args.regularizer + \") for alpha=%f and beta=%f\" % (args.alpha, args.beta))\n",
        "fig2.tight_layout(pad=2, h_pad=2, w_pad=2)\n",
        "\n",
        "axs2[0].set(ylabel='SVDD Loss')\n",
        "axs2[0].plot(list(range(1, no_epochs + 1)), svdd_losses)\n",
        "axs2[0].grid()\n",
        "\n",
        "axs2[1].set(ylabel='Regularizer Loss')\n",
        "axs2[1].plot(list(range(1, no_epochs + 1)), reg_losses)\n",
        "axs2[1].grid()\n",
        "\n",
        "axs2[2].set(ylabel='Total Loss')\n",
        "axs2[2].plot(list(range(1, no_epochs + 1)), total_losses)\n",
        "axs2[2].grid()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig2.savefig(\"losses_\" + filename, dpi=1000)\n",
        "#files.download(\"losses_\" + filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxRZ9K-ngMx0"
      },
      "source": [
        "def em(t, t_max, volume_support, s_unif, s_X, n_generated):\n",
        "    EM_t = np.zeros(t.shape[0])\n",
        "    n_samples = s_X.shape[0]\n",
        "    s_X_unique = np.unique(s_X)\n",
        "    EM_t[0] = 1.\n",
        "    for u in s_X_unique:\n",
        "        # if (s_unif >= u).sum() > n_generated / 1000:\n",
        "        EM_t = np.maximum(EM_t, 1. / n_samples * (s_X < u).sum() -\n",
        "                          t * (s_unif < u).sum() / n_generated\n",
        "                          * volume_support)\n",
        "    amax = np.argmax(EM_t <= t_max) + 1\n",
        "    if amax == 1:\n",
        "        print('\\n failed to achieve t_max \\n')\n",
        "        amax = -1\n",
        "    AUC = auc(t[:amax], EM_t[:amax])\n",
        "    return AUC, EM_t, amax\n",
        "\n",
        "\n",
        "def mv(axis_alpha, volume_support, s_unif, s_X, n_generated):\n",
        "    n_samples = s_X.shape[0]\n",
        "    s_X_argsort = s_X.argsort()\n",
        "    mass = 0\n",
        "    cpt = 0\n",
        "    u = s_X[s_X_argsort[0]]\n",
        "    mv = np.zeros(axis_alpha.shape[0])\n",
        "    for i in range(axis_alpha.shape[0]):\n",
        "        # pdb.set_trace()\n",
        "        while mass < axis_alpha[i]:\n",
        "            cpt += 1\n",
        "            u = s_X[s_X_argsort[cpt-1]]\n",
        "            mass = 1. / n_samples * cpt  # sum(s_X > u)\n",
        "        mv[i] = float((s_unif <= u).sum()) / n_generated * volume_support\n",
        "    return auc(axis_alpha, mv), mv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2DBOimgOLe"
      },
      "source": [
        "def compute_em_mv_aucs(X):\n",
        "\n",
        "    n_generated = 1000\n",
        "    alpha_min = 0.001\n",
        "    alpha_max = 0.999\n",
        "    t_max = 0.9\n",
        "\n",
        "    n_samples = len(X)\n",
        "    n_features = 1\n",
        "\n",
        "    lim_inf = X.min()\n",
        "    lim_sup = X.max()\n",
        "    volume_support = lim_sup - lim_inf\n",
        "    t = np.arange(0, 100 / volume_support, 0.01 / volume_support)\n",
        "    axis_alpha = np.arange(alpha_min, alpha_max, 0.001)\n",
        "    unif = np.random.uniform(lim_inf, lim_sup, size=(n_generated, n_features))\n",
        "\n",
        "    auc_em, _, _ = em(t, t_max, volume_support, unif, X, n_generated)\n",
        "\n",
        "    auc_mv, _ = mv(axis_alpha, volume_support,\n",
        "                                    unif, X, n_generated)\n",
        "    return auc_em, auc_mv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaH077xH400d"
      },
      "source": [
        "def customscore(X):\n",
        "    inliers = np.sort(X)[:int(0.95*len(X))]\n",
        "    inlier_median = np.median(inliers)\n",
        "    full_median = np.median(X)\n",
        "\n",
        "    score = np.mean((X - full_median)**2) / np.mean((inliers - inlier_median)**2)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqlmn2pGnar0"
      },
      "source": [
        "def rsquared(X):\n",
        "    outliers = np.sort(X)[int(0.95*len(X)):]\n",
        "    inliers = np.sort(X)[:int(0.95*len(X))]\n",
        "    inlier_median = np.median(inliers)\n",
        "    outlier_median = np.median(outliers)\n",
        "    full_median = np.median(X)\n",
        "\n",
        "    score = (np.sum((inliers - inlier_median)**2) + np.sum((outliers - outlier_median)**2))/np.sum((X - full_median)**2)\n",
        "    return 1-score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haHYYkGcxp0g"
      },
      "source": [
        "def xb(X):\n",
        "    outliers = np.sort(X)[int(0.95*len(X)):]\n",
        "    inliers = np.sort(X)[:int(0.95*len(X))]\n",
        "    inlier_median = np.median(inliers)\n",
        "    outlier_median = np.median(outliers)\n",
        "    \n",
        "    score = (np.sum((inliers - inlier_median)**2) + np.sum((outliers - outlier_median)**2))/ (len(X)*(inlier_median - outlier_median)**2)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56KF3M9P6PpT"
      },
      "source": [
        "rsquareds = []\n",
        "customs = []\n",
        "xbs = []\n",
        "ems = []\n",
        "mvs = []\n",
        "for dists in distlist:\n",
        "    em_auc, mv_auc = compute_em_mv_aucs(dists)\n",
        "    ems.append(em_auc)\n",
        "    mvs.append(mv_auc)\n",
        "    customs.append(customscore(dists))\n",
        "    rsquareds.append(rsquared(dists))\n",
        "    xbs.append(xb(dists))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQyHEcyJ81du"
      },
      "source": [
        "fig3, axs3 = plt.subplots(4)\n",
        "axs3[0].set(ylabel='Average Precision', ylim=((0,1)))\n",
        "axs3[0].plot(list(range(0, no_epochs + 1)), aps)\n",
        "axs3[0].grid()\n",
        "\n",
        "axs3[1].set(xlabel='Epochs', ylabel='Distances')\n",
        "axs3[1].scatter(xs, ys, s=1, color='blue')\n",
        "axs3[1].scatter(xs2, ys2, s=1, color='red')\n",
        "axs3[1].grid()\n",
        "\n",
        "#axs2[4].set(ylabel='custom')\n",
        "#axs2[4].plot(list(range(len(customs))), customs)\n",
        "#axs2[4].grid()\n",
        "\n",
        "#axs3[2].set(ylabel='R Squared')\n",
        "#axs3[2].plot(list(range(len(rsquareds))), rsquareds)\n",
        "#axs3[2].grid()\n",
        "\n",
        "axs3[2].set(ylabel='Total Loss')\n",
        "axs3[2].plot(list(range(len(total_losses))), total_losses)\n",
        "axs3[2].grid()\n",
        "\n",
        "axs3[3].set(ylabel='XB')\n",
        "axs3[3].plot(list(range(len(xbs))), xbs)\n",
        "axs3[3].grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig3.savefig(\"cluster\"+filename, dpi=1000)\n",
        "files.download(\"cluster\"+filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j75iSrAatHV0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}